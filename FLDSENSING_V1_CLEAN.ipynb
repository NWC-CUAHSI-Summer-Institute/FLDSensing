{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a461f990-2bea-4f4f-a1ac-907404e953f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 0: Import FLDPLN Library and Establish Folder Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a4269-f807-49c7-b0cc-fbd25a974413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import pathlib\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import os\n",
    "\n",
    "# import the mapping module in the fldpln package\n",
    "# import DASK libraries for parallel mapping\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import visualize\n",
    "\n",
    "# import the mapping and gauge modules from the fldpln package\n",
    "from fldpln.mapping import *\n",
    "from fldpln.gauge import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from shapely.geometry import Point\n",
    "from rasterio.transform import from_origin\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.stats import rankdata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c53ff2",
   "metadata": {},
   "source": [
    "### Set Mapping Parameters\n",
    "Select the tiled library folder for study area and select the library to be mapped. \n",
    "\n",
    "Setup the map folder (i.e., outMapFolderName) which is under the output folder and comtains all inundation depth maps. Additional settings include whether to mosaic tiles as single COG file and whether use a Dask local cluster to speed up the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4bf6aa-bdf1-4ed9-8495-0b1b244ddf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish working directory. All notebooks, FLDPLN python library, and study area terrain data should be in the directory\n",
    "work_dir = pathlib.Path().resolve()\n",
    "print('Working directory: ', work_dir)\n",
    "\n",
    "# tiled library folder\n",
    "libFolder =  'C:/Users/hobbe/Documents/Thesis/fldpln/Data/verdigris_10m_v8/tiled_snz_library'\n",
    "print('Tile library: ', libFolder)\n",
    "\n",
    "# libraries to be mapped\n",
    "allLibNames = ['lib_fldsensing']\n",
    "\n",
    "#Filled DEM location\n",
    "\n",
    "fil_path = str(work_dir / r'C:/Users/hobbe/Documents/Thesis/fldpln/Data/verdigris_10m_v8/bil/fil.bil')\n",
    "\n",
    "\n",
    "# Set output folder for location of all map outputs\n",
    "outputFolder = 'C:/Users/hobbe/Documents/Thesis/fldpln/Data/verdigris_10m_v8/maps'\n",
    "\n",
    "# set up map folder\n",
    "outMapFolderName = 'testingSpecial5'\n",
    "# Create folders for storing temp and output map files\n",
    "outMapFolder,scratchFolder = CreateFolders(outputFolder,'scratch',outMapFolderName)\n",
    "print('Output maps stored in: ', outputFolder)\n",
    "\n",
    "if not os.path.exists(outputFolder):\n",
    "    os.makedirs(outputFolder)\n",
    "\n",
    "# whether mosaci tiles as a single COG\n",
    "mosaicTiles = True #True #False\n",
    "\n",
    "# Using LocalCluster by default\n",
    "useLocalCluster = False # This doesn't work on my office desktop though it works fine on KBS server\n",
    "numOfWorkers = round(0.8*os.cpu_count())\n",
    "numOfWorkers = 6\n",
    "print(f'Number of workers: {numOfWorkers}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69005780-e150-4eae-911f-bd201beb6a83",
   "metadata": {},
   "source": [
    "## Note\n",
    "In FLDPLN code, the procedure first snap and interpolate stage between FSPs and later, given the value, map the corresponding FPPs. For the method proposed, technically this interpolation step should be performed after the RS edge selection and later, deriving the stage that generates the DoF close to zero at the FPP.\n",
    "\n",
    "*Steps*\n",
    "\n",
    "1. Load \"clean edge\" raster\n",
    "2. Snap raster to .bil or similar\n",
    "3. Extract X,Y coordinates of edge pixels into a df\n",
    "4. Open FLDPLN_tiled_tile_index.csv to extract info of .snz tiles and edge pixel locations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84523d1e-e4d3-4044-8b97-dc709bbd8d6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 1: Reproject true edge tif file into the same projection and alignment with FLDPLN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b97979-e6e5-4592-8def-a2eee3b712d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snap_raster(src_path, ref_path, dst_path):\n",
    "    '''\n",
    "    This function reprojects and alligns the true edge tiff file into the same projection with FLDPLN library and save the new projected\n",
    "    file.\n",
    "    \n",
    "    args:\n",
    "        src_path (str): Directory of true edge tif file \n",
    "        ref_path (str): Directory of FLDPLN DEM bil file\n",
    "        dst_path (str): Directory of new reprojected true edge file should be \n",
    "    '''\n",
    "    with rasterio.open(ref_path) as ref_src:\n",
    "        ref_transform = ref_src.transform\n",
    "        ref_crs = ref_src.crs\n",
    "        ref_width = ref_src.width\n",
    "        ref_height = ref_src.height\n",
    "\n",
    "    with rasterio.open(src_path) as src:\n",
    "        profile = src.profile\n",
    "        profile.update({\n",
    "            'crs': ref_crs,\n",
    "            'transform': ref_transform,\n",
    "            'width': ref_width,\n",
    "            'height': ref_height\n",
    "        })\n",
    "\n",
    "        with rasterio.open(dst_path, 'w', **profile) as dst:\n",
    "            for i in range(1, src.count + 1):\n",
    "                reproject(\n",
    "                    source=rasterio.band(src, i),\n",
    "                    destination=rasterio.band(dst, i),\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=ref_transform,\n",
    "                    dst_crs=ref_crs,\n",
    "                    resampling=Resampling.nearest\n",
    "                )\n",
    "    print('New true edge raster saved at: %s'%(dst_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd5dc2-f432-40b7-805f-7e08f2544a59",
   "metadata": {},
   "source": [
    "### Note: If the reprojected true edge raster has not been made, uncomment the cell below and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b54d54-4edc-4708-92ff-2ea5484bde49",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = str(work_dir / r'C:/Users/hobbe/Documents/Thesis/fldpln/Data/verdigris_10m_v8/RS/hecrasedge.tif')\n",
    "dst_path = str(work_dir / r'C:/Users/hobbe/Documents/Thesis/fldpln/Data/verdigris_10m_v8/RS/hecrasedge_align.tif')    \n",
    "ref_path = str(work_dir / r'C:/Users/hobbe/Documents/Thesis/fldpln/Data/verdigris_10m_v8/bil/dem.bil')\n",
    "snap_raster(src_path, ref_path, dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a505b-9141-451d-98f5-c09209fde07b",
   "metadata": {},
   "source": [
    "### Else, run the cell below to establish alligned, reprojected true edge raster path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a88b5-f1c8-410a-b657-a923dc71732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_path = 'C:/Users/hobbe/Documents/Thesis/fldpln/Data/verdigris_10m_v8/RS/hecrasedge_align.tif' \n",
    "print(dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3618c9-9c84-4fc4-a3bb-1f0b666135f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 2: Build DataFrame of true edge pixels and their associated index in FLDPLN library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7922d-e8b1-47d2-9a89-65455c5a531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coordinates(raster_path, value):\n",
    "    '''\n",
    "    This function transforms true edge raster into dataframe of X/Y coordinates.\n",
    "    \n",
    "    args:\n",
    "        raster_path (str): Directory of true edge raster (should be reprojected and aligned)\n",
    "        value (int/float): Value that represents true edge (usually 1)\n",
    "\n",
    "    return:\n",
    "        pd.DataFrame\n",
    "    '''\n",
    "    \n",
    "    with rasterio.open(raster_path) as src:\n",
    "        data = src.read(1)\n",
    "        transform = src.transform\n",
    "\n",
    "    rows, cols = np.where(data == value)\n",
    "    xs, ys = rasterio.transform.xy(transform, rows, cols)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'X_coord': xs,\n",
    "        'Y_coord': ys\n",
    "    })\n",
    "    return df\n",
    "\n",
    "## Function to find the tile ID for a given FPP coordinate\n",
    "def find_tile_id_and_indices(x, y, tile_index_df, cell_size):\n",
    "    '''\n",
    "    This function searchs the tile ID and row/col value of a given FPP X/Y coordinate based on the information \n",
    "    of Max/Min X/Y of each tile.  \n",
    "    \n",
    "    args:\n",
    "        x (int/float): X coordinate (Lon)\n",
    "        y (int/float): Y coordinate (Lat)\n",
    "        tile_index_df (pd.DataFrame): tile information dataframe\n",
    "        cell_size (int/float): spatial resolution in meter\n",
    "\n",
    "    return:\n",
    "        int, int, int: tile ID, FPP col index, FPP row index\n",
    "    '''\n",
    "    for idx, row in tile_index_df.iterrows():\n",
    "        if row['TileMinX'] < x < row['TileMaxX'] and row['TileMinY'] < y < row['TileMaxY']:\n",
    "            fpp_col = int((x - (row['FppMinX'] + hcs)) / cell_size)\n",
    "            fpp_row = int(((row['FppMaxY'] - hcs) - y) / cell_size)\n",
    "            return row['TileId'], fpp_col, fpp_row\n",
    "    return None, None, None\n",
    "\n",
    "def extract_elevation_from_dem(filledDem_path, coordinates_df):\n",
    "    '''\n",
    "    Extract elevation values from a DEM for a given set of coordinates.\n",
    "\n",
    "    args:\n",
    "        dem_path (str): Path to the DEM file.\n",
    "        coordinates_df (pd.DataFrame): DataFrame containing X/Y coordinates (columns: 'X_coord', 'Y_coord').\n",
    "\n",
    "    return:\n",
    "        Input DataFrame with an additional 'FPP_Elevation' column.\n",
    "    '''\n",
    "    with rasterio.open(filledDem_path) as dem:\n",
    "        # Prepare coordinates as a list of tuples\n",
    "        coords = list(zip(coordinates_df['X_coord'], coordinates_df['Y_coord']))\n",
    "\n",
    "        # Sample elevation values from the DEM\n",
    "        elevation_values = [\n",
    "            val[0] if val else np.nan  # Handle None values returned by rasterio.sample\n",
    "            for val in dem.sample(coords)\n",
    "        ]\n",
    "\n",
    "        # Add the elevation values to the DataFrame\n",
    "        coordinates_df['FPP_Elevation'] = elevation_values\n",
    "\n",
    "    return coordinates_df\n",
    "\n",
    "# Create dataframe of True edge coordinate\n",
    "value_to_extract = 1\n",
    "RS_edges = extract_coordinates(dst_path, value_to_extract)\n",
    "\n",
    "## Load the tile index CSV file\n",
    "tile_index_path = str(pathlib.Path(libFolder) / allLibNames[0] / 'FLDPLN_tiled_tile_index.csv')\n",
    "tile_index_df = pd.read_csv(tile_index_path)\n",
    "cell_size = 10\n",
    "hcs = cell_size / 2\n",
    "tile_index_df.head()\n",
    "\n",
    "## Apply the function to each row in the coordinates DataFrame to obtain tile ID and row/col\n",
    "RS_edges[['TileId', 'FppCol', 'FppRow']] = RS_edges.apply(\n",
    "    lambda row: pd.Series(find_tile_id_and_indices(row['X_coord'], row['Y_coord'], tile_index_df,cell_size)), axis=1)\n",
    "RS_edges_with_elevation = extract_elevation_from_dem(fil_path, RS_edges)\n",
    "## Display the first few rows of the DataFrame with Tile IDs and FppCols/FppRows\n",
    "RS_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b534b3c-f82a-44e2-962c-c720156d8818",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 3: Read tile .snz file and query DTFs relationship of true edge FPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c265c6c0-6a06-4bcc-b146-3117b407ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to get the FspIds and Dtfs for a specific coordinate from the loaded tile data\n",
    "def get_fspids_and_dtfs_for_coordinate(fpp_col, fpp_row, tile_id, tile_data):\n",
    "    '''\n",
    "    This function saves all FspIDs, Dtfs , and the FilledDepth values that correspond to the FPP   \n",
    "    \n",
    "    args:\n",
    "        fpp_col (int): Column index of the FPP in the tile\n",
    "        fpp_row (int): Row index of the FPP in the tile\n",
    "        tile_id (int): The id of the tile\n",
    "        tile_data (dict): Dictionary that contain all tile data from the .snz files \n",
    "\n",
    "    return:\n",
    "        list, list, list: FspID list, Dtf list, FilledDepth list\n",
    "    '''\n",
    "    \n",
    "    tile_df = tile_data.get(tile_id)\n",
    "    if tile_df is not None:\n",
    "        results = tile_df[(tile_df['FppCol'] == fpp_col) & (tile_df['FppRow'] == fpp_row)]\n",
    "        if not results.empty:\n",
    "            return results['FspId'].tolist(), results['Dtf'].tolist(), results['FilledDepth'].tolist()\n",
    "    return [], [], []\n",
    "\n",
    "## Get unique tile IDs needed for the coordinates\n",
    "dummy = RS_edges['TileId'].dropna().unique()\n",
    "unique_tile_ids = dummy.astype(int)\n",
    "print('Tiles to load:'+str(unique_tile_ids))\n",
    "\n",
    "# Load data for each required tile\n",
    "\n",
    "tile_data = {}\n",
    "for tile_id in unique_tile_ids:\n",
    "    snz_path = str(pathlib.Path(libFolder) / allLibNames[0] / str('FLDPLN_tiled_%s.snz'%(tile_id)))\n",
    "    tile_data[tile_id] = pd.read_parquet(snz_path)\n",
    "\n",
    "## Apply the data extraction function to each row in the coordinates DataFrame\n",
    "RS_edges[['FspIds', 'Dtfs', 'FilledDepth']] = RS_edges.apply(\n",
    "    lambda row: pd.Series(get_fspids_and_dtfs_for_coordinate(row['FppCol'], row['FppRow'], row['TileId'], tile_data)), axis=1)\n",
    "\n",
    "RS_edges.head()\n",
    "\n",
    "## Filter the Edge Dataframe to remove edges with no FSP relationship. Add a column that count number of FSPs for each FPP\n",
    "RS_edges_filtered = RS_edges.copy()\n",
    "RS_edges_filtered['NumsFSP'] = RS_edges['FspIds'].map(len)\n",
    "RS_edges_filtered = RS_edges_filtered.drop(RS_edges_filtered[RS_edges_filtered['NumsFSP'] == 0].index).reset_index(drop = True)\n",
    "RS_edges_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b5738-eded-4de8-9e01-c6ee7e4621b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 4: Create GeoDataframes of the true edge dataframe and of the FSPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b356cd77-0d88-4ae5-9014-bb3b52de4414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Map a given Fpp and its Fsp by creating GeoDataframe\n",
    "gdfRS_edges_filtered = gpd.GeoDataFrame(RS_edges_filtered, geometry=gpd.points_from_xy(RS_edges_filtered['X_coord'], RS_edges_filtered['Y_coord']))\n",
    "gdfRS_edges = gpd.GeoDataFrame(RS_edges, geometry=gpd.points_from_xy(RS_edges['X_coord'], RS_edges['Y_coord']))\n",
    "\n",
    "# Set the coordinate reference system (CRS) to EPSG 26914 for NAD UTM 14N. Change if required\n",
    "# It is possible to load .prj from folder for this procedure but we set it here for convenience\n",
    "\n",
    "gdfRS_edges_filtered = gdfRS_edges_filtered.set_crs(epsg=26914)\n",
    "\n",
    "# Save the maximum depth/DTF of each FPP \n",
    "max_depth = gdfRS_edges_filtered.Dtfs.apply(lambda x:np.max(x))\n",
    "gdfRS_edges_filtered['max_depth'] = max_depth\n",
    "gdfRS_edges_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ecbc6-f5c5-479f-b9c2-1ec96b82ec49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Note: To reduce outliers you must remove pixels that have an unrealist Depth To Flood value. To do this an average max depth can be taken and whatever pixel is above the average will be filtered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac69e8-631a-45d2-9780-869ac3fb9ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "average = sum(max_depth)/len(max_depth)\n",
    "\n",
    "print(\"Pixels above following number will be masked out: \", average)\n",
    "max_dtf_threshold = average\n",
    "\n",
    "gdfRS_edges_filtered = gdfRS_edges_filtered[gdfRS_edges_filtered['max_depth'] < max_dtf_threshold].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f41c86-9c8a-4c73-ba03-bd8e082535a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the FSP info GeoDataframe\n",
    "fsp_path = str(pathlib.Path(libFolder) / allLibNames[0] / 'fsp_info.csv')\n",
    "print(libFolder)\n",
    "fsp_info = pd.read_csv(fsp_path)\n",
    "\n",
    "print(fsp_path)\n",
    "fsp_gdf = gpd.GeoDataFrame(fsp_info, geometry=gpd.points_from_xy(fsp_info['FspX'], fsp_info['FspY']))\n",
    "## Set the coordinate reference system (CRS) to EPSG 26914\n",
    "fsp_gdf = fsp_gdf.set_crs(epsg=26914, inplace=True)\n",
    "fspGDFPlot=fsp_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c5b61c-b6e9-4bfc-b090-c216cf6b57ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 5: Plot Geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f2e1f2-ba08-4a10-af10-ce90e4a92586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT FSP river profile and FPP at true edge pixel from RS\n",
    "\n",
    "## Normalize the FspId_count for color mapping\n",
    "# norm = Normalize(vmin=gdfRS_edges_filtered['NumsFSP'].min(), vmax=gdfRS_edges_filtered['NumsFSP'].max())\n",
    "norm = Normalize(vmin=gdfRS_edges_filtered['max_depth'].min(), vmax=gdfRS_edges_filtered['max_depth'].max())\n",
    "cmap = plt.cm.viridis\n",
    "\n",
    "## Plot the data\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 6))\n",
    "\n",
    "# gdfRS_edges_filtered.plot(column='NumsFSP', cmap=cmap, markersize=100, ax=ax, legend=False, norm=norm)\n",
    "gdfRS_edges_filtered.plot(column='max_depth', cmap=cmap, markersize=100, ax=ax, legend=False, norm=norm)\n",
    "\n",
    "fsp_gdf[ (fsp_gdf.StrOrd == 1)|(fsp_gdf.StrOrd == 2) | (fsp_gdf.StrOrd == 3)|(fsp_gdf.StrOrd == 4) | (fsp_gdf.StrOrd == 5)|(fsp_gdf.StrOrd == 6) | (fsp_gdf.StrOrd == 7)| (fsp_gdf.StrOrd == 8)].plot(ax=ax, color='green', markersize=2, label='Fsp')\n",
    "\n",
    "for i in unique_tile_ids:\n",
    "    row = tile_index_df.loc[tile_index_df['TileId'] == i, ['TileMinX', 'TileMaxX', 'TileMinY', 'TileMaxY']]\n",
    "    if not row.empty:\n",
    "        minx,maxx,miny,maxy = row.iloc[0]\n",
    "\n",
    "    rectangle = Polygon([(minx, miny), (maxx, miny), (maxx, maxy), (minx, maxy), (minx, miny)])\n",
    "\n",
    "    # Create a GeoDataFrame\n",
    "    rec = gpd.GeoDataFrame([1], geometry=[rectangle], crs=\"EPSG:26914\")\n",
    "    rec.plot(ax=ax, edgecolor='black', markersize=1, facecolor='none')\n",
    "\n",
    "sm = ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Max DTF')\n",
    "\n",
    "ax.set_title('Fpps with Max DTF')\n",
    "ax.set_xlabel('X Coordinate (EPSG 26914)')\n",
    "ax.set_ylabel('Y Coordinate (EPSG 26914)')\n",
    "\n",
    "plt.show()\n",
    "gdfRS_edges_filtered.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81442b-c727-4788-bedf-31c5bab18413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5.1 - Map a given Fpp and all Fsp related to it\n",
    "## Function to plot the user-provided point and related Fsp points\n",
    "\n",
    "def plot_user_point_and_fsps(user_point_index,gdf,fsp_gdf):\n",
    "    user_point = gdf.iloc[user_point_index]\n",
    "    related_fsp_ids = user_point['FspIds']\n",
    "    related_fsps = fsp_gdf[fsp_gdf['FspId'].isin(related_fsp_ids)]\n",
    "    \n",
    "    stream_order_colors = {\n",
    "        1: 'darkgreen',\n",
    "        2: 'blue',\n",
    "        3: 'orange',\n",
    "        4: 'purple',\n",
    "        5: 'darkred',\n",
    "        6: 'pink',\n",
    "        7: 'gray',\n",
    "        8: 'black',\n",
    "        9:'royalblue',\n",
    "        10:'brown',\n",
    "        11:'yellow',\n",
    "        12:'darkviolet',\n",
    "        13:'coral',\n",
    "        14:'aqua',\n",
    "        15:'dodgerblue',\n",
    "        16:'grey',\n",
    "        17:'salmon',\n",
    "        18:'olive',\n",
    "        19:'turquoise',\n",
    "        20:'gold',\n",
    "    }\n",
    "    # Plot the data\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "    for str_ord, color in stream_order_colors.items():\n",
    "        # Filter Fsp points based on stream order and plot\n",
    "        fsp_subset = fsp_gdf[fsp_gdf['StrOrd'] == str_ord]\n",
    "        fsp_subset.plot(ax=ax, color=color, markersize=2, label=f'Stream Order {str_ord}(Color: {color})')\n",
    "    # Plot the user-provided point\n",
    "    gdf.plot(ax=ax, color='cyan', markersize=5, label='RS Fpp(Color: Cyan)')\n",
    "    \n",
    "    user_point_gdf = gpd.GeoDataFrame([user_point], geometry=[user_point.geometry])\n",
    "    user_point_gdf.plot(ax=ax, color='red', markersize=50, label=f'User Provided Point (Color: red)')\n",
    "    \n",
    "    # Plot the related Fsp points\n",
    "    related_fsps.plot(ax=ax, color='lawngreen', markersize=25, label='Related Fsp (Color: Lime Green)')\n",
    "    \n",
    "    # Set plot title and labels\n",
    "    ax.set_title('Edge Pixels and Related Fsp Points')\n",
    "    ax.set_xlabel('X Coordinate (EPSG 26914)')\n",
    "    ax.set_ylabel('Y Coordinate (EPSG 26914)')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "print(len(gdfRS_edges_filtered))\n",
    "plot_user_point_and_fsps(0,gdfRS_edges_filtered,fspGDFPlot) #Change the number of the pixel here, default is 1\n",
    "fspGDFPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098269e7-ae1c-4e68-91ce-11a925083314",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 6: Assign Dof to FSP from RS true edge "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13a80ae-89ed-47ef-8129-5f67a9a5dc4d",
   "metadata": {},
   "source": [
    "## Note: In our study, we filtered out FPP at edges that have DTFs relationship higher than 10. Customize the threshold n in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332bb785-d7e7-46d2-ae1d-103ac7e44a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a threshold of NumofFSPs here for a subset of edges relationship. Can be used if number of FSPS causes program slowdown\n",
    "#n = 10\n",
    "gdfRS_edges_match_full = gdfRS_edges_filtered.copy()\n",
    "\n",
    "# Run this instead to have no limitation of NumofFSP\n",
    "n = gdfRS_edges_match_full.NumsFSP.max() \n",
    "\n",
    "gdfRS_edges_match = gdfRS_edges_match_full[gdfRS_edges_match_full.NumsFSP <= n].reset_index(drop = True) # Limit to edge with <= n Fsps\n",
    "gdfRS_edges_match.Dtfs = gdfRS_edges_match.Dtfs.apply(lambda x:np.round(x,1)) # Round the dtfs by 1 decimal to reduce possible \n",
    "gdfRS_edges_match.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping FspId to its coordinates from FspGDF\n",
    "fsp_coord_dict = fsp_gdf[['FspId', 'FspX', 'FspY', 'FilledElev']].set_index('FspId').to_dict('index')\n",
    "#print(fsp_coord_dict)\n",
    "# Function to get coordinates for a list of FspIds\n",
    "def get_fsp_coordinates(fsp_ids):\n",
    "    fsp_coords = []\n",
    "    for fsp_id in fsp_ids:\n",
    "        if fsp_id in fsp_coord_dict:\n",
    "            coords = fsp_coord_dict[fsp_id]\n",
    "            fsp_coords.append((coords['FspX'], coords['FspY']))\n",
    "    return fsp_coords\n",
    "def get_fsp_filledElevations(fsp_ids):\n",
    "    fsp_elevations = []\n",
    "    for fsp_id in fsp_ids:\n",
    "        if fsp_id in fsp_coord_dict:\n",
    "            elevation = fsp_coord_dict[fsp_id]\n",
    "            fsp_elevations.append((elevation['FilledElev']))\n",
    "    return fsp_elevations\n",
    "    \n",
    "# Add new columns for FSP coordinates\n",
    "gdfRS_edges_match_full['FSP_Coordinates'] = gdfRS_edges_match_full['FspIds'].apply(get_fsp_coordinates)\n",
    "\n",
    "# Add new coloumn for FSP elevation\n",
    "gdfRS_edges_match_full['FSP_Elevation'] = gdfRS_edges_match_full['FspIds'].apply(get_fsp_filledElevations)\n",
    "\n",
    "# Split the coordinates into separate X and Y lists\n",
    "gdfRS_edges_match_full['FSP_X_Coords'] = gdfRS_edges_match_full['FSP_Coordinates'].apply(lambda x: [coord[0] for coord in x])\n",
    "gdfRS_edges_match_full['FSP_Y_Coords'] = gdfRS_edges_match_full['FSP_Coordinates'].apply(lambda x: [coord[1] for coord in x])\n",
    "\n",
    "gdfRS_edges_match_full = gdfRS_edges_match_full.drop('FSP_Coordinates', axis=1, errors='ignore')\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "gdfRS_edges_match_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d03a35-595a-492d-a11d-aecb55394445",
   "metadata": {},
   "source": [
    "# Step 6\n",
    "### Note: There are two ways to assign Dof of FSP. One way is to use the median of all possible DTFs options for each FSP based on their relationships to FPP flood edge. Another way is to going through all possible combination of FSP Dof assignment and find the combination that provides the lowest flood depth across all flood edge pixels. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9a4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth(fsp,dtf,dof):\n",
    "    '''\n",
    "    This function calculates the flood depth at a certain FPP given the Flood Source Pixel (FSP) that can flood \n",
    "    it, the Depth To Flood (DTF) relationship, and the Depth of Flood (DOF) or stage of the FSP. The flood depth\n",
    "    is calculated as the maximum of (DOF - DTF) for each FSP. \n",
    "    \n",
    "    If flood depth is less than 0 (no FSP can flood the FPP), return nan\n",
    "    \n",
    "    args:\n",
    "        fsp (np.arary/list): FSP list\n",
    "        dtf (np.arary/list): corresponding DTF of the FSP\n",
    "        dof (np.array/list): corresponding DOF/stage of the FSP\n",
    "        \n",
    "    return:\n",
    "        float: flood depth value\n",
    "        nan: if FPP is not flooded\n",
    "    '''\n",
    "    fsp, dtf, dof = np.array(fsp), np.array(dtf), np.array(dof)\n",
    "    \n",
    "    depth = np.max(dof - dtf)\n",
    "    #print(f\"{depth:.2f} = {dof} - {dtf}\")\n",
    "    \n",
    "    if depth >= 0:\n",
    "        return depth\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def filter_close_values(input_dict, threshold):\n",
    "    '''\n",
    "    This function reduces the number of options for each FSPs to reduce computational time. Given a certain threshold, \n",
    "    no two values within each list are closer than the threshold.\n",
    "    \n",
    "    args:\n",
    "        input_dict (dict): Dictionary of key:value being FspID (int): Dof options (list)\n",
    "        threshold (float): Threshold to remove values close to each other\n",
    "    \n",
    "    return:\n",
    "        dict: filtered out FSP Dof functions \n",
    "    '''\n",
    "    \n",
    "    filtered_dict = {}\n",
    "    for key, values in input_dict.items():\n",
    "        # Sort values in descending order\n",
    "        sorted_values = sorted(values, reverse=True)\n",
    "        filtered_values = []\n",
    "        for value in sorted_values:\n",
    "            if all(abs(value - fv) > threshold for fv in filtered_values):\n",
    "                filtered_values.append(value)\n",
    "        \n",
    "        filtered_dict[key] = filtered_values\n",
    "    \n",
    "    return filtered_dict\n",
    "\n",
    "\n",
    "def num_of_combination(possible_fsp_dtf):\n",
    "    '''\n",
    "    This function calculate the number of possible combinations for FSP assignments \n",
    "    \n",
    "    args:\n",
    "        possible_fsp_dtf (dict): Dictionary of key:value being FspID (int): Dof options (list)\n",
    "        \n",
    "    return:\n",
    "        int: number of combinations\n",
    "    '''\n",
    "    a = 1\n",
    "    for key, value in possible_fsp_dtf.items():\n",
    "        a = a*len(value)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91dc9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setting Maximum Combinations and Initializing `best_case`\n",
    "max_combinations = 20000  # Maximum combinations to avoid excessive computation\n",
    "best_case = {}  # Store the best-case DOF for each FSP\n",
    "all_combinations_df = pd.DataFrame(columns=['Combination_Index', 'Edge_Index', 'FSP_ID', 'Flood_Depth', 'Distance', 'Combined_Rank'])\n",
    "final_results_df = pd.DataFrame(columns=['Edge_Index', 'Best_FSP', 'Dof', 'Flood_Depth', 'Distance', 'Combined_Rank'])\n",
    "final_records = []\n",
    "fsp_assignment_count = defaultdict(int)\n",
    "\n",
    "\n",
    "# 2. Iterate through Different Limits of FSP Count per Edge\n",
    "for i in range(gdfRS_edges_match_full.NumsFSP.min(), n + 1):\n",
    "    print(f\"Num of FSPs <= {i}\")\n",
    "    \n",
    "    # 3. Filter Data by FSP Count and Round `Dtfs` Values\n",
    "    gdfRS_edges_match = gdfRS_edges_match_full[gdfRS_edges_match_full.NumsFSP <= i].reset_index(drop=True)\n",
    "    gdfRS_edges_match.Dtfs = gdfRS_edges_match.Dtfs.apply(lambda x: np.round(x, 1))  # Reduce unique combinations\n",
    "    \n",
    "    # 4. Generate Possible DTF Values for Each FSP\n",
    "    possible_fsp_dtf = defaultdict(set)\n",
    "    for fspids, dtfs in zip(gdfRS_edges_match['FspIds'], gdfRS_edges_match['Dtfs']):\n",
    "        for fspid, dtf in zip(fspids, dtfs):\n",
    "            possible_fsp_dtf[fspid].add(dtf)\n",
    "\n",
    "    possible_fsp_dtf = {k: list(v) for k, v in possible_fsp_dtf.items()}\n",
    "    \n",
    "    for key, value in best_case.items():\n",
    "        possible_fsp_dtf[key] = [value]\n",
    "\n",
    "    # Calculate initial number of combinations\n",
    "    num_choice = num_of_combination(possible_fsp_dtf)\n",
    "    print(f\"There are {num_choice} options of FSP DOF\")\n",
    "\n",
    "    # Reduce options if combinations exceed the threshold\n",
    "    threshold = 0.1\n",
    "    while num_choice > max_combinations:\n",
    "        possible_fsp_dtf = filter_close_values(possible_fsp_dtf, threshold)\n",
    "        num_choice = num_of_combination(possible_fsp_dtf)\n",
    "        print(f\"Applied {np.round(threshold, 1)} threshold. There are {num_choice} options of FSP DOF\")\n",
    "        threshold += 0.1\n",
    "\n",
    "    # Generate combinations\n",
    "    fsp = list(possible_fsp_dtf.keys())\n",
    "    dtf_options = list(possible_fsp_dtf.values())\n",
    "    combinations = [dict(zip(fsp, combo)) for combo in product(*dtf_options)]\n",
    "\n",
    "    # 5. Initialize Optimization Variables\n",
    "    total_edge_depth_optimize = [np.inf]\n",
    "    idx_optimize = np.nan\n",
    "    best_fsp_per_edge = {}\n",
    "\n",
    "    # 6. Find the Best Combination\n",
    "    for combo_idx, combination in enumerate(combinations):\n",
    "        total_edge_depth = []\n",
    "        edge_condition = True\n",
    "\n",
    "        # Calculate flood depth for each edge pixel\n",
    "        for j in range(gdfRS_edges_match.shape[0]):\n",
    "            fsp = gdfRS_edges_match.loc[j, 'FspIds']\n",
    "            dtf = gdfRS_edges_match.loc[j, 'Dtfs']\n",
    "            dof = [combination[key] for key in fsp]\n",
    "\n",
    "            flood_depth = get_depth(fsp, dtf, dof)\n",
    "            if np.isnan(flood_depth):  # Stop if combination leads to unflooded edge\n",
    "                edge_condition = False\n",
    "                break\n",
    "            else:\n",
    "                total_edge_depth.append(flood_depth)\n",
    "\n",
    "        if not edge_condition:\n",
    "            continue\n",
    "\n",
    "        # Update optimal combination if it minimizes total flood depth\n",
    "        if sum(total_edge_depth) < sum(total_edge_depth_optimize):\n",
    "            total_edge_depth_optimize = total_edge_depth\n",
    "            idx_optimize = combo_idx\n",
    "\n",
    "    # Update `best_case` with the optimal combination\n",
    "    best_case = combinations[idx_optimize]\n",
    "    print(f\"Best case FSP combination: {best_case}\")\n",
    "    depth_weight = 1\n",
    "    distance_weight = 1 \n",
    "    elevation_weight = 1\n",
    "    #epsilon = 1e-6 \n",
    "    # 7. Rank FSPs Based on Flood Depth and Distance\n",
    "    for j in range(gdfRS_edges_match.shape[0]):\n",
    "        fsp_ids = gdfRS_edges_match.loc[j, 'FspIds']\n",
    "        dtfs = gdfRS_edges_match.loc[j, 'Dtfs']\n",
    "        edge_x, edge_y = gdfRS_edges_match.loc[j, 'X_coord'], gdfRS_edges_match.loc[j, 'Y_coord']\n",
    "        fsp_x_coords = gdfRS_edges_match.loc[j, 'FSP_X_Coords']\n",
    "        fsp_y_coords = gdfRS_edges_match.loc[j, 'FSP_Y_Coords']\n",
    "        fpp_elevations = gdfRS_edges_match.loc[j, 'FPP_Elevation']\n",
    "        fsp_elevations = gdfRS_edges_match.loc[j, 'FSP_Elevation']\n",
    "\n",
    "        fsp_ranks = []\n",
    "        for idx, fsp_id in enumerate(fsp_ids):\n",
    "            dtf = dtfs[idx]\n",
    "            fsp_x, fsp_y = fsp_x_coords[idx], fsp_y_coords[idx]\n",
    "            fsp_elevation = fsp_elevations[idx]\n",
    "            dof = best_case.get(fsp_id, None)\n",
    "            if dof is None:\n",
    "                continue\n",
    "\n",
    "            flood_depth = max(0, dof - dtf)\n",
    "            #print(f\"{flood_depth:.2f} = {dof} - {dtf}\")\n",
    "\n",
    "            xdistance = abs(((edge_x - fsp_x) ** 2 + (edge_y - fsp_y) ** 2) ** 0.5)\n",
    "            ydistance = fsp_elevation\n",
    "            weighted_depth =  (flood_depth) * depth_weight\n",
    "            weighted_distance = (1/ xdistance) * distance_weight \n",
    "            weighted_elevation = (1/ ydistance) * elevation_weight \n",
    "\n",
    "            fsp_ranks.append((fsp_id, weighted_depth, weighted_distance, weighted_elevation))\n",
    "        \n",
    "        # Rank by flood depth and distance\n",
    "        flood_depths = [rank[1] for rank in fsp_ranks]\n",
    "        distances = [rank[2] for rank in fsp_ranks]\n",
    "        vertical_distance = [rank[3] for rank in fsp_ranks]\n",
    "\n",
    "        flood_ranks = np.argsort(np.argsort(flood_depths)) + 1\n",
    "        distance_ranks = np.argsort(np.argsort(distances)) + 1\n",
    "        elevation_ranks = np.argsort(np.argsort(-np.array(vertical_distance))) + 1\n",
    "        combined_ranks_list = []\n",
    "        for k in range(len(fsp_ranks)):\n",
    "            combined_value = (\n",
    "                depth_weight*flood_ranks[k] +\n",
    "                distance_weight*distance_ranks[k] +\n",
    "                elevation_weight*elevation_ranks[k]\n",
    "            )\n",
    "            combined_ranks_list.append(combined_value)\n",
    "\n",
    "\n",
    "        best_index = np.argmin(combined_ranks_list)\n",
    "        best_fsp = fsp_ranks[best_index][0]\n",
    "        best_fsp_per_edge[j] = best_fsp\n",
    "        best_depth = fsp_ranks[best_index][1]\n",
    "        best_distance = fsp_ranks[best_index][2]\n",
    "        best_elevation_difference = fsp_ranks[best_index][3]\n",
    "        best_combined_rank = combined_ranks_list[best_index]\n",
    "        \n",
    "        final_records.append({\n",
    "            'Edge_Index': j,\n",
    "            'Best_FSP': best_fsp,\n",
    "            'Dof': best_case.get(best_fsp),\n",
    "            'Flood_Depth': best_depth,\n",
    "            'Distance': best_distance,\n",
    "            'Elevation_Distance': best_elevation_difference,\n",
    "            'Combined_Rank': best_combined_rank\n",
    "        })   \n",
    "    final_results_df = pd.DataFrame(final_records)\n",
    "\n",
    "    print(f\"{len(final_results_df)} edges pixels optimized\")\n",
    "    print(f\"Minimum flood depth: {final_results_df['Flood_Depth'].min():.2f}\")\n",
    "    print(f\"Median flood depth: {final_results_df['Flood_Depth'].median():.2f}\")\n",
    "    print(f\"Mean flood depth: {final_results_df['Flood_Depth'].mean():.2f}\")\n",
    "    print(f\"Max flood depth: {final_results_df['Flood_Depth'].max():.2f}\")\n",
    "    print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a78487-4812-4008-a5c6-15a873383360",
   "metadata": {},
   "source": [
    "## Step 6.2: Go through all possible combinations\n",
    "### Note: We use the set of true edges with the lowest NumofFSP to assign associated FSP Dofs and then iteratively move on to the set of true edge with higher NumofFSP. Fsps being assigned in each iteration doesn't change after each iteration. \n",
    "\n",
    "### Set the maximum number of possible combinations for each iteration. When the total number of possible combinations exceed this value, the FSP DOF dictionary is filtered with a 0.1 incremental threshold to remove Dofs options that are close to each other\n",
    "\n",
    "1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41085ca0-2963-43fd-87a9-0149450c22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #1. Setting Maximum Combinations and Initializing `best_case`\n",
    "# max_combinations = 100000 # The maximum number of combinations to avoid excessive computations, if there are a lot of edge pixels and computation times are too long reduce this number\n",
    "# best_case = {} #intilize dictionary to store the bestcase DOF for each FSP\n",
    "\n",
    "# #2. Iterate through Different Limits of FSP Count per Edge  \n",
    "# for i in range(gdfRS_edges_match_full.NumsFSP.min(),n + 1): \n",
    "#     print(\"NumofFSPs <= %s\"%(i))\n",
    "#     #3. Filter Data by FSP Count and Round `Dtfs` Values \n",
    "#     #limit the dataset to edges with Number of FSPs <=i and rest index\n",
    "#     gdfRS_edges_match = gdfRS_edges_match_full[gdfRS_edges_match_full.NumsFSP <= i].reset_index(drop = True) # Limit to edge with <= n Fsps\n",
    "\n",
    "#     #Round DTFs values to one decimal to reduce unique combinations \n",
    "#     gdfRS_edges_match.Dtfs = gdfRS_edges_match.Dtfs.apply(lambda x:np.round(x,1))\n",
    "    \n",
    "#     #4. Generate Possible DTF Values for Each FSP \n",
    "#     #Create a dictionary of FSP IDs and possible DTFs for each ID\n",
    "#     possible_fsp_dtf = defaultdict(set)\n",
    "#     for fspids, dtfs in zip(gdfRS_edges_match['FspIds'], gdfRS_edges_match['Dtfs']): \n",
    "#         for fspid, dtf in zip(fspids, dtfs):\n",
    "#             possible_fsp_dtf[fspid].add(dtf)\n",
    "    \n",
    "#     #Convert each set to of possible DTF values to a list for easy combination generation\n",
    "#     possible_fsp_dtf = {k: list(v) for k, v in possible_fsp_dtf.items()}\n",
    "#     print('Total number of possible Fsp are %s'%(len(possible_fsp_dtf)))\n",
    "\n",
    "#     #Update possible FSP choices with optimized values from the previous iteration\n",
    "#     for key,value in best_case.items():\n",
    "#         possible_fsp_dtf[key] = [value]\n",
    "\n",
    "#     #Calculate the original number of combinations\n",
    "#     num_choice = num_of_combination(possible_fsp_dtf)\n",
    "#     print('There are %s options of FSP DOF '%(num_choice))\n",
    "\n",
    "#     #Reduce options if the number of combinations exceeds the threshold \n",
    "#     threshold = 0.1\n",
    "#     while num_choice > max_combinations:\n",
    "#         possible_fsp_dtf = filter_close_values(possible_fsp_dtf, threshold)\n",
    "#         num_choice = num_of_combination(possible_fsp_dtf)\n",
    "#         print('Applied %s threshold. There are %s options of FSP DOF '%(np.round(threshold, 1), num_choice))\n",
    "#         threshold += 0.1\n",
    "\n",
    "#     #Generarte all possible combinations of FSP-DOF values\n",
    "#     fsp = list(possible_fsp_dtf.keys())\n",
    "#     dtf_options = list(possible_fsp_dtf.values())\n",
    "#     combinations = [dict(zip(fsp, combo)) for combo in product(*dtf_options)]\n",
    "\n",
    "#     #Initialize variables for optimize edge depth \n",
    "#     total_edge_depth_optimize = [np.inf] \n",
    "#     idx_optimize = np.nan #Store index of optimal combination \n",
    "    \n",
    "#     # Find the best combination that minimize flood depth at edge\n",
    "#     for i in range(len(combinations)): #Iterate through each combinations of FSPs' DOF\n",
    "#         total_edge_depth = []# Accumulate flood depths for each combination\n",
    "#         edge_condition = True # Track wheteher the combiation is valid\n",
    "        \n",
    "#         # Iterate through edge pixels to get the depth for each combination\n",
    "#         for j in range( gdfRS_edges_match.shape[0]): \n",
    "#             fsp = gdfRS_edges_match.loc[j, 'FspIds']\n",
    "#             dtf = gdfRS_edges_match.loc[j, 'Dtfs']  \n",
    "#             dof = [combinations[i][key] for key in fsp]\n",
    "\n",
    "#             # Calculate flood depth; stop if a non-flooded edge is ecnountered\n",
    "#             flood_depth = get_depth(fsp,dtf,dof)\n",
    "#             if np.isnan(flood_depth): # Stop calculating when the scenario produces a true edge that is not flooded\n",
    "#                 edge_condition = False\n",
    "#                 break\n",
    "#             else:\n",
    "#                 total_edge_depth.append(flood_depth)\n",
    "\n",
    "#         # Skip to the next combination if this one is invalid         \n",
    "#         if not edge_condition: # Go to the next combination\n",
    "#             continue\n",
    "#         else:\n",
    "#             # Update optimal combination if current one has lower total flood depth \n",
    "#             if sum(total_edge_depth) < sum(total_edge_depth_optimize):\n",
    "#                 total_edge_depth_optimize = total_edge_depth\n",
    "#                 idx_optimize = i\n",
    "\n",
    "#     # Update best-case FSP-DOF values with the current optimal combination          \n",
    "#     best_case = combinations[idx_optimize]\n",
    "#     print('%s FSP optimized' %(len(best_case)))\n",
    "#     print('Total edge pixels: %s'%(gdfRS_edges_match.shape[0]))\n",
    "#     print('Edge pixels that dont get flooded: %s'%(np.count_nonzero(np.isnan(total_edge_depth_optimize))))\n",
    "#     print('Median flood depth at edge pixels: %s'%(np.nanmedian(total_edge_depth_optimize).round(2)))\n",
    "#     print('Mean flood depth at edge pixels: %s'%(np.nanmean(total_edge_depth_optimize).round(2)))\n",
    "#     print('Max flood depth at edge pixels: %s'%(np.nanmax(total_edge_depth_optimize).round(2)))\n",
    "#     print('-------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e242b-9a97-4fb2-9786-b7b925522bee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 7: SNAP to FSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8865bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_fsp_ids = list(best_fsp_per_edge.values())\n",
    "\n",
    "# Step 2: Create a new DataFrame from the original `fsp_gdf` with only selected FSPs\n",
    "# Use the best-case FSPs' DOF values for the mapping\n",
    "sol = fsp_gdf.set_index('FspId').loc[selected_fsp_ids].copy()\n",
    "sol['Dof'] = sol.index.map(best_case)  # Map the optimized DOF values from `best_case`\n",
    "\n",
    "# Step 3: Display the updated DataFrame\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  \n",
    "#     display(sol)\n",
    "\n",
    "# Step 4: Analyze and print summary statistics\n",
    "# Print the counts of each `StrOrd` value\n",
    "print(sol['StrOrd'].value_counts())\n",
    "\n",
    "# Find the most common `StrOrd` value\n",
    "highest_value = sol['StrOrd'].value_counts().index[0]\n",
    "print(f\"Most common StrOrd: {highest_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6648e-87cc-458e-8f6f-9d21a43f1592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the FSP Geodataframe with Dof being the optimized Dof derived from either way above\n",
    "# sol = fsp_gdf.set_index('FspId').loc[best_case.keys()]\n",
    "# sol['Dof'] = sol.index.map(best_case)\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  \n",
    "#     display(sol)\n",
    "\n",
    "# print(sol['StrOrd'].value_counts())\n",
    "\n",
    "# highest_value = sol['StrOrd'].value_counts().index[0]\n",
    "# print(highest_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b1443-37d3-4525-8c43-703578182694",
   "metadata": {},
   "source": [
    "### Note: In our study, we only kept assign FSPs on the mainstem. Skip this cell below if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2dc1a8-8c27-4b56-83bc-0e3405d8fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sol = sol[(sol['StrOrd'] == highest_value)] # replace highest_value with any stream order number to select a specific stream order\n",
    "\n",
    "gaugeFspDf = sol[['FspX', 'FspY', 'StrOrd', 'DsDist', 'SegId', 'FilledElev','Dof']].reset_index(drop = True)\n",
    "gaugeFspDf.insert(0, 'lib_name', 'lib_fldsensing')\n",
    "display(gaugeFspDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e45fd-3c89-4d26-ae90-2fb01852dda5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 8: Interpolate FSP's DOF\n",
    "\n",
    "Here we interpolate the DOF for all the FSPs between the gauge-FSPs using their DOF calculated from previous step. The interpolation uses stream orders and starts from low stream order (i.e., main streams) to high stream order (i.e., tributatried). Either horizontal or vertical (by defaut) interpolation can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a2c97-d037-46d4-9612-1cf2f9841c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find libs with snapped gauges. They are the actual libs to map\n",
    "libs2Map = gaugeFspDf['lib_name'].drop_duplicates().tolist()\n",
    "print(libs2Map)\n",
    "\n",
    "# prepare the DF for storing interpolated FSP DOF\n",
    "fspDof = pd.DataFrame(columns=['LibName','FspId','Dof'])\n",
    "\n",
    "# prepare DFs for saving interpolated FSPs and their segment IDs\n",
    "fspCols = fspInfoColumnNames + ['Dof']\n",
    "segIdCols = ['SegId','LibName']\n",
    "fsps = pd.DataFrame(columns=fspCols)\n",
    "segIds =pd.DataFrame(columns=segIdCols)\n",
    "\n",
    "# map each library\n",
    "for libName in libs2Map:\n",
    "    # interpolate DOF for the gauges\n",
    "    # print('Interpolate FSP DOF using gauge DOF ...')\n",
    "    # fspIdDof = InterpolateFspDofFromGauge(libFolder,libName,gaugeFspDf) # 'V' by default\n",
    "    fspIdDof = InterpolateFspDofFromGauge(libFolder,libName,gaugeFspDf,weightingType='H') # horizontal interpolation\n",
    "    fspIdDof['LibName'] = libName\n",
    "    fspIdDofUse=fspIdDof.dropna()\n",
    "    display(fspIdDofUse)\n",
    "\n",
    "    # fspDof = fspDof.append(fspIdDof[['LibName','FspId','Dof']], ignore_index=True)\n",
    "    fspDof = pd.concat([fspDof,fspIdDofUse[['LibName','FspId','Dof']]], ignore_index=True)\n",
    "    fspDof['Dof'] = savgol_filter(fspDof['Dof'], 5001, 1)\n",
    "    # Keep interpolated FSP DOF for saving later\n",
    "    fspFile = os.path.join(libFolder, libName, fspInfoFileName)\n",
    "    fspDf = pd.read_csv(fspFile) \n",
    "    fspDf = pd.merge(fspDf,fspDof,how='inner',on=['FspId'])\n",
    "    # fsps = fsps.append(fspDf, ignore_index=True)\n",
    "    fsps = pd.concat([fsps,fspDf], ignore_index=True)\n",
    "    \n",
    "    # Keep FSP segment IDs for saving later\n",
    "    t =  pd.DataFrame(fspDf['SegId'].drop_duplicates().sort_values())\n",
    "    t['LibName'] = libName\n",
    "    # segIds = segIds.append(t, ignore_index=True)\n",
    "    segIds = pd.concat([segIds,t], ignore_index=True)\n",
    "\n",
    "\n",
    "# show interpolated FSPs with Dof\n",
    "print(fspDof)\n",
    "\n",
    "#\n",
    "# save interpolated FSP DOF and their segments for checking. This block of code should be commented out if no-checking needed\n",
    "#\n",
    "# Save DOF and segment IDs to CSV files\n",
    "FspDofFile = os.path.join(outputFolder, 'Interpolated_FSP_DOF.csv')\n",
    "SegIdFile = os.path.join(outputFolder, 'Interpolated_SegIds.csv')\n",
    "\n",
    "fsps.to_csv(FspDofFile, index=False)\n",
    "segIds.to_csv(SegIdFile, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d5e5894-252e-49e3-857b-affc4a7c8a46",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 9: Map Flood Inundation Depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0248ea1-b0b9-48ad-a093-2de1c55fc6f4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The process of generating inundation depth map happens here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64feb7-864d-46cf-98c6-d695e9681848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show mapping info\n",
    "print(f'Tiled FLDPLN library folder: {libFolder}')\n",
    "print(f'Map folder: {outMapFolder}')\n",
    "# Find libs needs mapping\n",
    "libs2Map = fspDof['LibName'].drop_duplicates().tolist()\n",
    "print(f'Libraries to map: {libs2Map}')\n",
    "\n",
    "# check running time\n",
    "startTimeAllLibs = time.time()\n",
    "\n",
    "# create a local cluster to speed up the mapping. Must be run inside \"if __name__ == '__main__'\"!!!\n",
    "if useLocalCluster:\n",
    "    # cluster = LocalCluster(n_workers=4,processes=False)\n",
    "    try:\n",
    "        print('Start a LocalCluster ...')\n",
    "        # NOTE: set worker space (i.e., local_dir) to a folder that the LocalCluster can access. When run the script through a scheduled task, \n",
    "        # the system uses C:\\Windows\\system32 by default, which a typical user doesn't have the access!\n",
    "        # cluster = LocalCluster(n_workers=numOfWorkers,memory_limit='32GB',local_dir=\"D:/projects_new/fldpln/tools\") # for KARS production server (192G RAM & 8 cores)\n",
    "        # cluster = LocalCluster(n_workers=numOfWorkers,processes=False) # for KARS production server (192G RAM & 8 cores)\n",
    "        cluster = LocalCluster(n_workers=numOfWorkers,memory_limit='8GB',local_dir=\"D:\\temp\") # for office desktop (64G RAM & 8 cores)\n",
    "        # print('Watch workers at: ',cluster.dashboard_link)\n",
    "        print(f'Number of workers: {numOfWorkers}')\n",
    "        client = Client(cluster)\n",
    "        # print scheduler info\n",
    "        # print(client.scheduler_info())\n",
    "    except:\n",
    "        print('Cannot create a LocalCLuster!')\n",
    "        useLocalCluster = False\n",
    "\n",
    "# dict to store lib processing time\n",
    "libTime={}\n",
    "\n",
    "# map each library\n",
    "for libName in libs2Map:\n",
    "    # check running time\n",
    "    startTime = time.time()\n",
    "    \n",
    "    print(fspDof)\n",
    "    # select the FSPs within the lib\n",
    "    fspIdDof = fspDof[fspDof['LibName']==libName][['FspId','Dof']]\n",
    "    print(fspIdDof)\n",
    "\n",
    "    # mapping flood depth\n",
    "    if useLocalCluster:\n",
    "        print(f'Map [{libName}] using LocalCLuster ...')\n",
    "        # generate a DAG\n",
    "        dag,dagRoot=MapFloodDepthWithTilesAsDag(libFolder,libName,'snappy',outMapFolder,fspIdDof,aoiExtent=None)\n",
    "        if dag is None:\n",
    "            tileTifs = None\n",
    "        else:\n",
    "            # visualize DAG\n",
    "            # visualize(dag)\n",
    "            # Compute DAG\n",
    "            tileTifs = client.get(dag, dagRoot)\n",
    "            if not tileTifs: # list is empty\n",
    "                tileTifs =  None\n",
    "    else:\n",
    "        print(f'Map {libName} ...')\n",
    "        tileTifs = MapFloodDepthWithTiles(libFolder,libName,'snappy',outMapFolder,fspIdDof,aoiExtent=None)\n",
    "    print(f'Actual mapped tiles: {tileTifs}')\n",
    "\n",
    "    # Mosaic all the tiles from a library into one tif file\n",
    "    if mosaicTiles and not(tileTifs is None):\n",
    "        print('Mosaic tile maps ...')\n",
    "        mosaicTifName = libName+'_'+outMapFolderName+'.tif'\n",
    "        # Simplest implementation, may crash with very large raster\n",
    "        MosaicGtifs(outMapFolder,tileTifs,mosaicTifName,keepTifs=False)\n",
    "    \n",
    "    # check time\n",
    "    endTime = time.time()\n",
    "    usedTime = round((endTime-startTime)/60,3)\n",
    "    libTime[libName] = usedTime\n",
    "    # print(f'{libName} processing time (minutes):', usedTime)\n",
    "\n",
    "# Show processing time\n",
    "# Individual lib processing time\n",
    "print('Individual library mapping time:', libTime)\n",
    "# total time\n",
    "endTimeAllLibs = time.time()\n",
    "print('Total processing time (minutes):', round((endTimeAllLibs-startTimeAllLibs)/60,3))\n",
    "\n",
    "#\n",
    "# Shutdown local clusters\n",
    "#\n",
    "if useLocalCluster:\n",
    "    print('Shutdown LocalCluster ...')\n",
    "    cluster.close()\n",
    "    client.shutdown()\n",
    "    client.close()\n",
    "    useLocalCluster = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3cafb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = r\"C:\\Users\\hobbe\\Documents\\Thesis\\fldpln\\Data\\verdigris_10m_v8\\maps\\Testing9\\lib_fldsensing_Testing9.tif\"\n",
    "\n",
    "raster = rasterio.open(fp)\n",
    "fig, ax = plt.subplots(figsize=(25,25))\n",
    "#image_hidden = ax.imshow(raster.read(1))\n",
    "image_hidden = ax.imshow(raster.read(1, masked=True))\n",
    "\n",
    "fig.colorbar(image_hidden, ax=ax)\n",
    "rasterio.plot.show(raster, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c72457d",
   "metadata": {},
   "source": [
    "### Turning the water surface elevator from the above FSPInfo folder into a raster\n",
    "\n",
    "To display water surface elevation changes when changing various parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b188e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "file_path = \"C:/Users/hobbe/Documents/Thesis/fldpln/Data/verdigris_10m_v8/maps/Interpolated_FSP_DOF.csv\"\n",
    "fsp_data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure StrOrd is numeric\n",
    "fsp_data['StrOrd'] = pd.to_numeric(fsp_data['StrOrd'], errors='coerce')\n",
    "\n",
    "\n",
    "# Calculate Water Surface Elevation (WSE) as FilledElev + Dof\n",
    "fsp_data['WSE'] = fsp_data['FilledElev'] + fsp_data['Dof']\n",
    "fsp_data['DsDist']=fsp_data['DsDist']\n",
    "#print(fsp_data)\n",
    "\n",
    "# Create a GeoDataFrame from the FSP coordinates\n",
    "geometry = [Point(xy) for xy in zip(fsp_data['FspX'], fsp_data['FspY'])]\n",
    "fsp_gdf = gpd.GeoDataFrame(fsp_data, geometry=geometry)\n",
    "\n",
    "# Set the CRS to EPSG:26914 (UTM Zone 14N)\n",
    "fsp_gdf.set_crs(epsg=26914, inplace=True)\n",
    "\n",
    "# Create an empty raster with the desired resolution (e.g., 10 meters)\n",
    "x_min, y_min, x_max, y_max = fsp_gdf.total_bounds\n",
    "resolution = 10  # Example resolution (10 meters)\n",
    "cols = int((x_max - x_min) / resolution) + 1  # +1 to include the edge\n",
    "rows = int((y_max - y_min) / resolution) + 1  # +1 to include the edge\n",
    "fspid_data = np.full((rows, cols), np.nan)\n",
    "\n",
    "# Create an empty array for the raster data\n",
    "raster_data = np.full((rows, cols), np.nan)\n",
    "#print(fsp_gdf)\n",
    "\n",
    "# Get the transform for the raster (defines the geo-referencing)\n",
    "transform = from_origin(x_min, y_max, resolution, resolution)\n",
    "\n",
    "# Fill in the raster array with WSE values at the FSP coordinates\n",
    "for idx, row in fsp_gdf.iterrows():\n",
    "    col = int((row.geometry.x - x_min) / resolution)\n",
    "    row_idx = int((y_max - row.geometry.y) / resolution)\n",
    "    \n",
    "    # Ensure indices are within bounds\n",
    "    if 0 <= row_idx < rows and 0 <= col < cols:\n",
    "        raster_data[row_idx, col] = row['WSE']\n",
    "        #fspid_data[row_idx, col] = row['FspId'] \n",
    "    else:\n",
    "        print(f\"Index out of bounds for coordinates: {row.geometry.x}, {row.geometry.y}\")\n",
    "\n",
    "# Define the raster's metadata\n",
    "raster_meta = {\n",
    "    'driver': 'GTiff',\n",
    "    'dtype': 'float32',\n",
    "    'nodata': np.nan,\n",
    "    'width': cols,\n",
    "    'height': rows,\n",
    "    'count': 1,\n",
    "    'crs': fsp_gdf.crs,\n",
    "    'transform': transform\n",
    "}\n",
    "\n",
    "# Save the raster as a GeoTIFF\n",
    "print(os.getcwd())\n",
    "output_tiff_path = 'WSE_rasterWithTributaries.tif'\n",
    "with rasterio.open(output_tiff_path, 'w', **raster_meta) as dst:\n",
    "    dst.write(raster_data.astype(np.float32), 1)\n",
    "\n",
    "print(f\"Raster saved as {output_tiff_path}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "tributaries = fsp_gdf[fsp_gdf['StrOrd'] > 1]\n",
    "mainstem = fsp_gdf[fsp_gdf['StrOrd'] == 1]\n",
    "\n",
    "print(fsp_gdf)\n",
    "fsp_gdf.to_csv('WSEWithTrib.csv')\n",
    "\n",
    "# Plot the mainstem and tributaries\n",
    "mainstem.plot(ax=ax, marker='o', color='blue', markersize=5)\n",
    "tributaries.plot(ax=ax, marker='x', color='red', markersize=5)\n",
    "\n",
    "# Add a colorbar for the water surface elevation (WSE) based on 'FilledElev'\n",
    "mainstem.plot(column='WSE', cmap='Reds', ax=ax, legend=True)\n",
    "\n",
    "# Add titles and labels\n",
    "ax.set_title('Mainstem FSP Water Surface Elevation with Tributaries')\n",
    "ax.set_xlabel('X Coordinate')\n",
    "ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed506ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dstData = fsp_gdf.query('StrOrd ==1')['DsDist']\n",
    "print(dstData)\n",
    "wseData = fsp_gdf.query('StrOrd ==1')['WSE']\n",
    "print(wseData)\n",
    "filledElevData = fsp_gdf.query('StrOrd ==1')['FilledElev']\n",
    "f = plt.figure()\n",
    "plt.title('Mainstem FSP Water Surface Elevation with Tributaries')\n",
    "plt.xlabel('Downstream Distance')\n",
    "plt.ylabel('Y Coordinate')\n",
    "f.set_figwidth(10)\n",
    "f.set_figheight(5)\n",
    "plt.plot(dstData,wseData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
