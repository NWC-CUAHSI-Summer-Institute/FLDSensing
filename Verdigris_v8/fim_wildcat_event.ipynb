{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wildcat Creek 2018 Labor Day Flood Inundation Mapping\n",
    "\n",
    "This notebook uses the Wildcat Creek (near Manhattan, KS) Labor Day flood event in 2018 to demonstarte how to map the flood event using FLDPLN tiled library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules\n",
    "\n",
    "Import necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import FLDPLN Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool/script folder\n",
    "fldplnToolFolder = r'Z:\\FLDPLN\\tools_os' # tool development folder, has the latest version\n",
    "\n",
    "# Add the tool/script folder to sys.path to access fldpln modules\n",
    "sys.path.append(fldplnToolFolder) \n",
    "# fldpln module\n",
    "from fldpln import *\n",
    "from fldpln_library import *\n",
    "from fldpln_gauge import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Input Tiled Library and Output Folders\n",
    "\n",
    "Here we setup the folder under which tiled libraries (organized as folders) are located. We also setup the output folder (i.e., outputFolder) under which a map folder and a 'scratch' folder are created. The map folder, which is specified later, comtains all inundation depth maps. The scratch folder stores temporary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiled library folder\n",
    "libFolder =  r'E:\\fldpln\\sites\\wildcat_10m_3dep\\tiled_snz_library'\n",
    "\n",
    "# libraries to be mapped\n",
    "allLibNames = ['lib2']\n",
    "\n",
    "# Set output folder\n",
    "outputFolder = r'E:\\fldpln\\sites\\wildcat_10m_3dep\\maps'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Gauge Stage and Calculate Gauge Depth of Flow (DOF)\n",
    "\n",
    "Here we obtain and prepare flood event stages from stream gauges. The stage at a gauge typically refers to the gauge's datum, which is not necessary of the stream bed elevation which is based on a certain vertical datum. In order to use gauge stage in a FLDPLN library, we need to make sure that gauge stage elevation (gauage + stage) and FSP's filled elevation are based on the same vertical datum. The depth of flow (DOF) at the FSP can then be calculated as the difference. The Wildcat Creek DEM and FLDPLN library are based on the NAVD88 vertical datum. So gauge stage elevations need to be based on the vertical datum too to calculate the DOFs at those gauges. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauge Stage from AHPS and USGS\n",
    "\n",
    "Both USGS and NWS AHPS maintain stream gauages which record past flood stages. There are three AHPS and USGS gauges ([WKCK1](https://water.weather.gov/ahps2/hydrograph.php?wfo=top&gage=wkck1), [MWCK1](https://water.weather.gov/ahps2/hydrograph.php?wfo=top&gage=MWCK1), [MSTK1](https://water.weather.gov/ahps2/hydrograph.php?wfo=top&gage=MSTK1)) on the Wildcat Creek that record the 2018 Labor Day flood event. Here we use the maximum Labor Day flood event stages at those gauges to map the maximum inundation extent and depth of the event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Event Stage from AHPS Historic Crests\n",
    "\n",
    "The flood stage for the 2018 Labor Day flood event in 2018 are availble as AHPS histroic crests at those gauges [WKCK1](https://water.noaa.gov/gauges/WKCK1), [MWCK1](https://water.weather.gov/ahps2/hydrograph.php?wfo=top&gage=MWCK1) and [MSTK1](https://water.weather.gov/ahps2/hydrograph.php?wfo=top&gage=MSTK1). Excel file wildcat_gauges_albers_meters.xlsx has several sheets which store both gauge information (for example, gauge datum) and the event statges with different gauge combinations. The key fields needed for those gauges are: stationid, x, y, and stage_elevation\n",
    "\n",
    "Note that most USGS and AHPS gauge stages are measured in feet and **Make sure that gauge coordinates are in the same coordinate system of the library and gauge stages are also in the same vertical unit of the library.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        stationid           x             y  stage_elevation\n",
      "0  06879805,WKCK1 -60735.0580  1.799635e+06       343.911936\n",
      "1  06879810,MWCK1 -54988.6141  1.796210e+06       325.907400\n",
      "2  06879815,MSTK1 -52277.2352  1.795783e+06       317.851536\n"
     ]
    }
   ],
   "source": [
    "# # Two downstream gauges \n",
    "# gaugeStageFileName = 'wildcat_gauges.xlsx' # KS LiDAR DEM in UTM with vertical unit in feet\n",
    "gaugeStageFileName = 'wildcat_gauges_albers_meters.xlsx' # 3DEP DEM in Albers with vertical unit in meters\n",
    "sheetName = 'ThreeGauges' # all 3 gauges\n",
    "# sheetName = 'TwoDsGauges' # 2 downstream gauges\n",
    "# sheetName = 'MSTK1' # the last downstream gauge used in HEC-RAS model\n",
    "\n",
    "# read gauge file\n",
    "gaugeStages = pd.read_excel(gaugeStageFileName, sheet_name=sheetName) \n",
    "# print(gaugeStages)\n",
    "\n",
    "# Need to calculate gauge stage elevation if necessary!\n",
    "\n",
    "# keep only necessary fields from gauges\n",
    "keptFields = ['stationid','x','y','stage_elevation']\n",
    "gaugeWithStageElevations = gaugeStages[keptFields]\n",
    "print(gaugeWithStageElevations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Event Stage from USGS NWIS\n",
    "\n",
    "We can also get event maximum stage directly from USGS NWIS to cehck the historic crests from AHPS. Note that the stages are in feet and we need to convert stages to stage elevation before using it in flood inundation mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lixi\\.conda\\envs\\fldpln\\Lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'waterservices.usgs.gov'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\lixi\\.conda\\envs\\fldpln\\Lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'waterservices.usgs.gov'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\lixi\\.conda\\envs\\fldpln\\Lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'nwis.waterservices.usgs.gov'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    stationid  stage_ft                     stage_time\n",
      "0    06879805      6.87  2018-09-02T00:00:00.000-05:00\n",
      "1    06879805      6.87  2018-09-02T00:15:00.000-05:00\n",
      "2    06879805      6.87  2018-09-02T00:30:00.000-05:00\n",
      "3    06879805      6.87  2018-09-02T00:45:00.000-05:00\n",
      "4    06879805      6.87  2018-09-02T01:00:00.000-05:00\n",
      "..        ...       ...                            ...\n",
      "812  06879815      5.73  2018-09-04T22:45:00.000-05:00\n",
      "813  06879815      5.73  2018-09-04T23:00:00.000-05:00\n",
      "814  06879815      5.72  2018-09-04T23:15:00.000-05:00\n",
      "815  06879815      5.72  2018-09-04T23:30:00.000-05:00\n",
      "816  06879815      5.71  2018-09-04T23:45:00.000-05:00\n",
      "\n",
      "[817 rows x 3 columns]\n",
      "  stationid  stage_ft                     stage_time\n",
      "0  06879805     25.97  2018-09-03T04:45:00.000-05:00\n",
      "1  06879810     28.29  2018-09-03T07:00:00.000-05:00\n",
      "2  06879815     25.18  2018-09-03T08:30:00.000-05:00\n"
     ]
    }
   ],
   "source": [
    "# Wildcat Creek 3 USGS gauges (in the order from upstream to downstream)\n",
    "usgsIds = ['06879805','06879810','06879815'] \n",
    "ahpsIds = ['WKCK1','MWCK1','MSTK1']\n",
    "\n",
    "# A period between two dates: Wildcat Creek Sep.3 2018 flood event\n",
    "instStages = GetUsgsGaugeStageFromWebService(usgsIds,startDate='2018-09-02',endDate='2018-09-04')\n",
    "print(instStages)\n",
    "\n",
    "# find the max stage within the time period\n",
    "maxStages = instStages.groupby(['stationid'],as_index=False).agg({'stage_ft':'max'})\n",
    "# find the most recent time with the max stage\n",
    "tdf = pd.merge(instStages, maxStages, how='inner', on=['stationid','stage_ft'])\n",
    "gaugeStagesFromNwis = tdf.groupby(['stationid'], as_index=False).agg({'stationid':'first','stage_ft':'first','stage_time':'max'})\n",
    "print(gaugeStagesFromNwis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Gauge Stage from the National Water Model and HAND\n",
    "\n",
    "HAND FIM uses NWM's discharge and turn it into stage. Here we use HAND reach stage to run FLDPLN for the event. Concepually, we turn reach stage into a synthetic gauge located at the either the mid-point or the outlet of the reach. Selecting the HAND reaches and sythteric gauge location is done by graduate student David Weiss manually for the Wildcat Creek example. Those sytheteic gauges can be treated as USGS/AHPS guages. The key fields needed are: stationid, x, y, and stage_elevation.  Note that we assume the HAND reach stage elevation is the same as the FLDPLN library DEM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic FSP gauges from NWC reach stage\n",
    "# gaugeStageFileName = 'wildcat_gauges.xlsx'\n",
    "# sheetName = 'ReachStageAsDof' \n",
    "gaugeStageFileName = 'wildcat_gauges_albers_meters.xlsx'\n",
    "# sheetName = 'ReachMedianStage' # HAND reach median stage as DOF\n",
    "sheetName = 'ReachOutletStage' # HAND reach outlet stage as DOF\n",
    "\n",
    "# read gauge file\n",
    "gaugeStages = pd.read_excel(gaugeStageFileName, sheet_name=sheetName) # 3 gauges\n",
    "print(gaugeStages)\n",
    "\n",
    "# Need to calculate gauge stage elevation if necessary!\n",
    "\n",
    "# keep only necessary fields from gauges\n",
    "keptFields = ['stationid','x','y','stage_elevation']\n",
    "gaugeWithStageElevations = gaugeStages[keptFields]\n",
    "print(gaugeWithStageElevations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snap Gauges to FSPs and Calculate Gauge DOF\n",
    "\n",
    "Here we snap gauges (with their stage elevation) to FLDPLN flood source pixels (FSPs), which are the stream pixels. Each snapped gauge FSP has a stream bed elevaltion, which is used to claculate the depth of flow/flood (DOF) at those FSPs. \n",
    "\n",
    "This process also identifies the FLDPLN libraries that the gauges belong to. Note that the same gauges can be snapped to more than one library as FLDPLN libraries may overlap and the overalpping FSPs may have different coordinates! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snap gauges to FSPs ...\n",
      "Number of gauges: 3\n",
      "   index       stationid           x             y  stage_elevation  \\\n",
      "0      0  06879805,WKCK1 -60735.0580  1.799635e+06       343.911936   \n",
      "1      1  06879810,MWCK1 -54988.6141  1.796210e+06       325.907400   \n",
      "2      2  06879815,MSTK1 -52277.2352  1.795783e+06       317.851536   \n",
      "\n",
      "   d2NearestFsp          FspX          FspY  StrOrd        DsDist  SegId  \\\n",
      "0      7.437232 -60738.205794  1.799628e+06     1.0  29037.232304    3.0   \n",
      "1      2.043149 -54988.205794  1.796208e+06     1.0  16424.448763    9.0   \n",
      "2      4.761831 -52278.205794  1.795788e+06     1.0   9202.762620   12.0   \n",
      "\n",
      "   FilledElev lib_name  \n",
      "0  338.434052     lib2  \n",
      "1  318.953552     lib2  \n",
      "2  311.347992     lib2  \n",
      "Number of snapped gauge FSPs: 3\n",
      "Libraries gauges snapped to: ['lib2']\n",
      "  lib_name          FspX          FspY  StrOrd        DsDist  SegId  \\\n",
      "0     lib2 -60738.205794  1.799628e+06     1.0  29037.232304    3.0   \n",
      "1     lib2 -54988.205794  1.796208e+06     1.0  16424.448763    9.0   \n",
      "2     lib2 -52278.205794  1.795788e+06     1.0   9202.762620   12.0   \n",
      "\n",
      "   FilledElev       Dof  \n",
      "0  338.434052  5.477884  \n",
      "1  318.953552  6.953848  \n",
      "2  311.347992  6.503544  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\FLDPLN\\tools_os\\fldpln.py:93: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  nearestP2Df = pd.concat([nearestP2Df,t],ignore_index=False)\n"
     ]
    }
   ],
   "source": [
    "# snap gauges to FSPs on-the-fly\n",
    "print('Snap gauges to FSPs ...')\n",
    "print(f'Number of gauges: {len(gaugeWithStageElevations.index)}')\n",
    "\n",
    "# FLDPLN libraries to whose FSPs gauges are sanpped. All the libraries by default but can be a subset\n",
    "libs2Map = ['lib2']\n",
    "\n",
    "# snap the gauges to FSPs. \n",
    "# Fields 'StrOrd','DsDist','SegId','FilledElev'are used for interpolating other FSP DOF\n",
    "# Note that 'lib_name','FspX', 'FspY' together uniquely identify a FSP (as there are overlapping FSPs between libraries)!\n",
    "gaugeFspDf = SnapGauges2Fsps(libFolder,libs2Map,gaugeWithStageElevations,snapDist=350,gaugeXField='x',gaugeYField='y',fspColumns=['FspX','FspY','StrOrd','DsDist','SegId','FilledElev']) \n",
    "print(gaugeFspDf)\n",
    "\n",
    "# calculate gauge FSP's DOF\n",
    "gaugeFspDf['Dof'] = gaugeFspDf['stage_elevation'] - gaugeFspDf['FilledElev']\n",
    "\n",
    "# keep only necessary columns for gauge FSPs\n",
    "gaugeFspDf = gaugeFspDf[['lib_name','FspX','FspY','StrOrd','DsDist','SegId','FilledElev','Dof']] # Note that 'lib_name','FspX', 'FspY' together uniquely identify a FSP!!!\n",
    "\n",
    "# show info\n",
    "print(f'Number of snapped gauge FSPs: {len(gaugeFspDf)}')\n",
    "# Find libs where the gauges are snapped to, and they are the actual libs to map\n",
    "libs2Map = gaugeFspDf['lib_name'].drop_duplicates().tolist()\n",
    "print(f'Libraries gauges snapped to: {libs2Map}')\n",
    "print(gaugeFspDf)\n",
    "\n",
    "#\n",
    "# save snapped gauges to CSV file for checking\n",
    "# gaugeFspDf.to_csv(os.path.join(outputFolder, 'SnappedGauges.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate FSP's DOF\n",
    "\n",
    "Here we interpolate the DOF for all the FSPs between the gauge-FSPs using their DOF calculated from previous step. The interpolation uses stream orders and starts from low stream order (i.e., main streams) to high stream order (i.e., tributatried). Either horizontal or vertical (by defaut) interpolation can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     LibName FspId       Dof\n",
      "0       lib2   150  5.230906\n",
      "1       lib2   151  5.237311\n",
      "2       lib2   152  5.241841\n",
      "3       lib2   153  5.248247\n",
      "4       lib2   154  5.252776\n",
      "...      ...   ...       ...\n",
      "1762    lib2  1912  0.629674\n",
      "1763    lib2  1913  0.445247\n",
      "1764    lib2  1914  0.314837\n",
      "1765    lib2  1915  0.184427\n",
      "1766    lib2  1916  0.000000\n",
      "\n",
      "[1767 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\FLDPLN\\tools_os\\fldpln.py:523: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  fspDof = pd.concat([fspDof,fspOrd],ignore_index=True)\n",
      "C:\\Users\\lixi\\AppData\\Local\\Temp\\ipykernel_26636\\3740564350.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  fspDof = pd.concat([fspDof,fspIdDof[['LibName','FspId','Dof']]], ignore_index=True)\n",
      "C:\\Users\\lixi\\AppData\\Local\\Temp\\ipykernel_26636\\3740564350.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  fsps = pd.concat([fsps,fspDf], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Find libs with snapped gauges. They are the actual libs to map\n",
    "libs2Map = gaugeFspDf['lib_name'].drop_duplicates().tolist()\n",
    "\n",
    "# prepare the DF for storing interpolated FSP DOF\n",
    "fspDof = pd.DataFrame(columns=['LibName','FspId','Dof'])\n",
    "\n",
    "# prepare DFs for saving interpolated FSPs and their segment IDs\n",
    "fspCols = fspInfoColumnNames + ['Dof']\n",
    "segIdCols = ['SegId','LibName']\n",
    "fsps = pd.DataFrame(columns=fspCols)\n",
    "segIds =pd.DataFrame(columns=segIdCols)\n",
    "\n",
    "# map each library\n",
    "for libName in libs2Map:\n",
    "    # interpolate DOF for the gauges\n",
    "    # print('Interpolate FSP DOF using gauge DOF ...')\n",
    "    # fspIdDof = InterpolateFspDofFromGauge(libFolder,libName,gaugeFspDf) # 'V' by default\n",
    "    fspIdDof = InterpolateFspDofFromGauge(libFolder,libName,gaugeFspDf,weightingType='H') # horizontal interpolation\n",
    "    fspIdDof['LibName'] = libName\n",
    "    # fspDof = fspDof.append(fspIdDof[['LibName','FspId','Dof']], ignore_index=True)\n",
    "    fspDof = pd.concat([fspDof,fspIdDof[['LibName','FspId','Dof']]], ignore_index=True)\n",
    "\n",
    "    # Keep interpolated FSP DOF for saving later\n",
    "    fspFile = os.path.join(libFolder, libName, fspInfoFileName)\n",
    "    fspDf = pd.read_csv(fspFile) \n",
    "    fspDf = pd.merge(fspDf,fspDof,how='inner',on=['FspId'])\n",
    "    # fsps = fsps.append(fspDf, ignore_index=True)\n",
    "    fsps = pd.concat([fsps,fspDf], ignore_index=True)\n",
    "    \n",
    "    # Keep FSP segment IDs for saving later\n",
    "    t =  pd.DataFrame(fspDf['SegId'].drop_duplicates().sort_values())\n",
    "    t['LibName'] = libName\n",
    "    # segIds = segIds.append(t, ignore_index=True)\n",
    "    segIds = pd.concat([segIds,t], ignore_index=True)\n",
    "\n",
    "# show interpolated FSPs with Dof\n",
    "print(fspDof)\n",
    "\n",
    "#\n",
    "# save interpolated FSP DOF and their segments for checking. This block of code should be commented out if no-checking needed\n",
    "#\n",
    "# Save DOF and segment IDs to CSV files\n",
    "FspDofFile = os.path.join(outputFolder, 'Interpolated_FSP_DOF.csv')\n",
    "SegIdFile = os.path.join(outputFolder, 'Interpolated_SegIds.csv')\n",
    "fsps.to_csv(FspDofFile, index=False)\n",
    "segIds.to_csv(SegIdFile, index=False)\n",
    "\n",
    "# # turn interpolated sgements into a shapefile\n",
    "# for libName in libs2Map:\n",
    "#     segShp = os.path.join(libFolder, libName, 'stream_orders.shp')\n",
    "#     segs = gpd.read_file(segShp)\n",
    "#     segs['LibName'] = libName\n",
    "#     # print(segs)\n",
    "#     # join by two fields: SegId and LibName\n",
    "#     segDf = pd.merge(segs,segIds,how='inner',on=['SegId','LibName'])\n",
    "#     # print(segDf)\n",
    "#     # write segments as a shapefile\n",
    "#     segDf.to_file(os.path.join(outputFolder, 'Interpolated_Segements.shp'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Flood Inundation Depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Mapping Parameters\n",
    "\n",
    "Setup the map folder (i.e., outMapFolderName) which is under the output folder and comtains all inundation depth maps. Additional settings include whether to mosaic tiles as single COG file and whether use a Dask local cluster to speed up the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of workers: 6\n"
     ]
    }
   ],
   "source": [
    "# set up map folder\n",
    "outMapFolderName = 'labor_day_2018_3_gauges'\n",
    "\n",
    "# Create folders for storing temp and output map files\n",
    "outMapFolder,scratchFolder = CreateFolders(outputFolder,'scratch',outMapFolderName)\n",
    "\n",
    "# whether mosaci tiles as a single COG\n",
    "mosaicTiles = True #True #False\n",
    "\n",
    "# Using LocalCluster by default\n",
    "useLocalCluster = False # This doesn't work on my office desktop though it works fine on KBS server\n",
    "numOfWorkers = round(0.8*os.cpu_count())\n",
    "numOfWorkers = 6\n",
    "print(f'Number of workers: {numOfWorkers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Inundation Depth\n",
    "\n",
    "The process of generating inundation depth map happens here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiled FLDPLN library folder: E:\\fldpln\\sites\\wildcat_10m_3dep\\tiled_snz_library\n",
      "Map folder: E:\\fldpln\\sites\\wildcat_10m_3dep\\maps\\labor_day_2018_3_gauges\n",
      "Libraries to map: ['lib2']\n",
      "Map lib2 ...\n",
      "Tiles need to be mapped: [1]\n",
      "Actual mapped tiles: ['E:\\\\fldpln\\\\sites\\\\wildcat_10m_3dep\\\\maps\\\\labor_day_2018_3_gauges\\\\lib2_tile_1.tif']\n",
      "Mosaic tile maps ...\n",
      "Individual library mapping time: {'lib2': 0.011}\n",
      "Total processing time (minutes): 0.011\n"
     ]
    }
   ],
   "source": [
    "# show mapping info\n",
    "print(f'Tiled FLDPLN library folder: {libFolder}')\n",
    "print(f'Map folder: {outMapFolder}')\n",
    "# Find libs needs mapping\n",
    "libs2Map = fspDof['LibName'].drop_duplicates().tolist()\n",
    "print(f'Libraries to map: {libs2Map}')\n",
    "\n",
    "# check running time\n",
    "startTimeAllLibs = time.time()\n",
    "\n",
    "# create a local cluster to speed up the mapping. Must be run inside \"if __name__ == '__main__'\"!!!\n",
    "if useLocalCluster:\n",
    "    # cluster = LocalCluster(n_workers=4,processes=False)\n",
    "    try:\n",
    "        print('Start a LocalCluster ...')\n",
    "        # NOTE: set worker space (i.e., local_dir) to a folder that the LocalCluster can access. When run the script through a scheduled task, \n",
    "        # the system uses C:\\Windows\\system32 by default, which a typical user doesn't have the access!\n",
    "        # cluster = LocalCluster(n_workers=numOfWorkers,memory_limit='32GB',local_dir=\"D:/projects_new/fldpln/tools\") # for KARS production server (192G RAM & 8 cores)\n",
    "        # cluster = LocalCluster(n_workers=numOfWorkers,processes=False) # for KARS production server (192G RAM & 8 cores)\n",
    "        cluster = LocalCluster(n_workers=numOfWorkers,memory_limit='8GB',local_dir=\"E:\\temp\") # for office desktop (64G RAM & 8 cores)\n",
    "        # print('Watch workers at: ',cluster.dashboard_link)\n",
    "        print(f'Number of workers: {numOfWorkers}')\n",
    "        client = Client(cluster)\n",
    "        # print scheduler info\n",
    "        # print(client.scheduler_info())\n",
    "    except:\n",
    "        print('Cannot create a LocalCLuster!')\n",
    "        useLocalCluster = False\n",
    "\n",
    "# dict to store lib processing time\n",
    "libTime={}\n",
    "\n",
    "# map each library\n",
    "for libName in libs2Map:\n",
    "    # check running time\n",
    "    startTime = time.time()\n",
    "    \n",
    "    # select the FSPs within the lib\n",
    "    fspIdDof = fspDof[fspDof['LibName']==libName][['FspId','Dof']]\n",
    "\n",
    "    # mapping flood depth\n",
    "    if useLocalCluster:\n",
    "        print(f'Map [{libName}] using LocalCLuster ...')\n",
    "        # generate a DAG\n",
    "        dag,dagRoot=MapFloodDepthWithTilesAsDag(libFolder,libName,'snappy',outMapFolder,fspIdDof,aoiExtent=None)\n",
    "        if dag is None:\n",
    "            tileTifs = None\n",
    "        else:\n",
    "            # visualize DAG\n",
    "            # visualize(dag)\n",
    "            # Compute DAG\n",
    "            tileTifs = client.get(dag, dagRoot)\n",
    "            if not tileTifs: # list is empty\n",
    "                tileTifs =  None\n",
    "    else:\n",
    "        print(f'Map {libName} ...')\n",
    "        tileTifs = MapFloodDepthWithTiles(libFolder,libName,'snappy',outMapFolder,fspIdDof,aoiExtent=None)\n",
    "    print(f'Actual mapped tiles: {tileTifs}')\n",
    "\n",
    "    # Mosaic all the tiles from a library into one tif file\n",
    "    if mosaicTiles and not(tileTifs is None):\n",
    "        print('Mosaic tile maps ...')\n",
    "        mosaicTifName = libName+'_'+outMapFolderName+'.tif'\n",
    "        # Simplest implementation, may crash with very large raster\n",
    "        MosaicGtifs(outMapFolder,tileTifs,mosaicTifName,keepTifs=False)\n",
    "    \n",
    "    # check time\n",
    "    endTime = time.time()\n",
    "    usedTime = round((endTime-startTime)/60,3)\n",
    "    libTime[libName] = usedTime\n",
    "    # print(f'{libName} processing time (minutes):', usedTime)\n",
    "\n",
    "# Show processing time\n",
    "# Individual lib processing time\n",
    "print('Individual library mapping time:', libTime)\n",
    "# total time\n",
    "endTimeAllLibs = time.time()\n",
    "print('Total processing time (minutes):', round((endTimeAllLibs-startTimeAllLibs)/60,3))\n",
    "\n",
    "#\n",
    "# Shutdown local clusters\n",
    "#\n",
    "if useLocalCluster:\n",
    "    print('Shutdown LocalCluster ...')\n",
    "    cluster.close()\n",
    "    client.shutdown()\n",
    "    client.close()\n",
    "    useLocalCluster = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fldpln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "acfdeded818a4df3f46e9c2992917120b0a663a2d6c4f04d9d79d65bad4d3fed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
