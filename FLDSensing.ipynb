{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a461f990-2bea-4f4f-a1ac-907404e953f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# FLDSensing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a4269-f807-49c7-b0cc-fbd25a974413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import pathlib\n",
    "from shapely.geometry import Polygon\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "\n",
    "# import the mapping module in the fldpln package\n",
    "# import DASK libraries for parallel mapping\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import visualize\n",
    "\n",
    "# import the mapping and gauge modules from the fldpln package\n",
    "from fldpln.mapping import *\n",
    "from fldpln.gauge import *\n",
    "\n",
    "import ee \n",
    "import geemap\n",
    "import ipywidgets as widgets\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import matplotlib.cm as cm\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from rasterio.transform import from_bounds\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from shapely.geometry import Point\n",
    "from rasterio.transform import from_origin\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7ee98",
   "metadata": {},
   "source": [
    "### Set Mapping Parameters\n",
    "User needs to define libFolder, allLibNames (the specific library that is to be inundated ), cell size of FLDPLN library, outputFolder, outMapFolderName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2db2636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish working directory. All notebooks, FLDPLN python library, and study area terrain data should be in the directory\n",
    "work_dir = pathlib.Path().resolve()\n",
    "print('Working directory: ', work_dir)\n",
    "\n",
    "# tiled library folder\n",
    "libFolder =  'C:/...' #path to folder containing all libraries\n",
    "print('Tile library: ', libFolder)\n",
    "\n",
    "# libraries to be mapped\n",
    "allLibNames = ['verdigriscaneyLib'] #specific library\n",
    "print(allLibNames)\n",
    "\n",
    "cell_size = 5 # the cell size (in meters) of the FLDPLN library. Incorrect cell size could cause improper results\n",
    "\n",
    "# Set output folder for location of all map outputs\n",
    "outputFolder = 'C:/...' #location of final FIM\n",
    "\n",
    "# Set up map folder\n",
    "outMapFolderName = 'verdigris'\n",
    "# Create folders for storing temp and output map files\n",
    "outMapFolder,scratchFolder = CreateFolders(outputFolder,'scratch',outMapFolderName)\n",
    "print('Output maps stored in: ', outputFolder)\n",
    "\n",
    "if not os.path.exists(outputFolder):\n",
    "    os.makedirs(outputFolder)\n",
    "\n",
    "# whether mosaic tiles as a single COG\n",
    "mosaicTiles = True #True #False\n",
    "\n",
    "# Using LocalCluster by default\n",
    "useLocalCluster = False # This doesn't work on my office desktop though it works fine on KBS server\n",
    "numOfWorkers = round(0.8*os.cpu_count())\n",
    "numOfWorkers = 6\n",
    "print(f'Number of workers: {numOfWorkers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32523cee",
   "metadata": {},
   "source": [
    "# GEEMAP clean flood edge extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3147b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map = geemap.Map()\n",
    "# clean_edge_np = None\n",
    "# ned_10m = ee.Image(\"USGS/3DEP/10m\")\n",
    "# nlcd_collection = ee.ImageCollection(\"USGS/NLCD_RELEASES/2019_REL/NLCD\")\n",
    "# landcoverDataset = nlcd_collection.filter(ee.Filter.eq('system:index', '2019')).first().select('landcover')\n",
    "\n",
    "# landcover_classes = {\n",
    "#     \"Open Water\": 11,\n",
    "#     \"Developed Open Space\": 21,\n",
    "#     \"Developed Low Intensity\": 22,\n",
    "#     \"Developed Medium Intensity\": 23,\n",
    "#     \"Developed High Intensity\": 24,\n",
    "#     \"Barren Land\": 31,\n",
    "#     \"Deciduous Forest\": 41,\n",
    "#     \"Evergreen Forest\": 42,\n",
    "#     \"Mixed Forest\": 43,\n",
    "#     \"Shrub/Scrub\": 52,\n",
    "#     \"Grassland/Herbaceous\": 71,\n",
    "#     \"Pasture/Hay\": 81,\n",
    "#     \"Cultivated Crops\": 82,\n",
    "#     \"Woody Wetlands\": 90,\n",
    "#     \"Emergent Herbaceous Wetlands\": 95\n",
    "# }\n",
    "\n",
    "# def clear_map(change=None):\n",
    "#     layers_to_remove = [\"Sentinel-2 RGB\", \"Selected Landcover Mask\", \"Water/Non-Water Classification\", \"Large Water Body Raster\", \"Combined\",\"Buffered\",\"\"]\n",
    "    \n",
    "#     for layer_name in layers_to_remove:\n",
    "#         if layer_name in [layer.name for layer in Map.layers]:\n",
    "#             Map.remove(layer_name)\n",
    "\n",
    "# def get_landcover_mask():\n",
    "#     selected_classes = [value for name, value in landcover_classes.items() if checkboxes[name].value]\n",
    "#     if selected_classes:\n",
    "#         mask = landcoverDataset.eq(selected_classes[0])\n",
    "#         for landCover in selected_classes[1:]:\n",
    "#             mask = mask.Or(landcoverDataset.eq(landCover))\n",
    "#         return mask\n",
    "#     return None\n",
    "\n",
    "# def maskByCS(image):\n",
    "#     CLEAR_THRESHOLD = 0.5\n",
    "#     qualBand = 'cs'\n",
    "#     clearThresh = CLEAR_THRESHOLD\n",
    "#     mask = image.select(qualBand).gte(clearThresh)\n",
    "#     masked_image = image.updateMask(mask)\n",
    "#     return masked_image  \n",
    "\n",
    "# def update_map(change=None):\n",
    "#     global clean_edge_resampled\n",
    "#     global latest_aoi\n",
    "#     global filename\n",
    "#     if \"Sentinel-2 RGB\" in Map.ee_layers:\n",
    "#         Map.remove(\"Sentinel-2 RGB\")\n",
    "#     if \"Selected Landcover Mask\" in Map.ee_layers:\n",
    "#         Map.remove(\"Selected Landcover Mask\")\n",
    "#     if \"Water/Non-Water Classification\" in Map.ee_layers:\n",
    "#         Map.remove(\"Water/Non-Water Classification\")\n",
    "#     if \"Large Water Body Raster\" in Map.ee_layers:\n",
    "#         Map.remove(\"Large Water Body Raster\")\n",
    "#     if \"Combined\" in Map.ee_layers:\n",
    "#         Map.remove(\"Combined\")\n",
    "#     if \"Buffered\" in Map.ee_layers:\n",
    "#         Map.remove(\"Buffered\")\n",
    "#     if \"Slope Max Mask\" in Map.ee_layers:\n",
    "#         Map.remove(\"Slope Max Mask\")\n",
    "#     if \"Slope Minimum Mask\" in Map.ee_layers:\n",
    "#         Map.remove(\"Slope Minimum Mask\")\n",
    "#     if \"NDVI Mask\" in Map.ee_layers:\n",
    "#         Map.remove(\"NDVI Mask\")\n",
    "#     if \"All Edge Pixels\" in Map.ee_layers:\n",
    "#         Map.remove(\"All Edge Pixels\")\n",
    "#     if \"Clean Edge Pixels\" in Map.ee_layers:\n",
    "#         Map.remove(\"Clean Edge Pixels\")\n",
    "\n",
    "#     drawn_features = Map.draw_features\n",
    "#     latest_aoi = drawn_features[0].geometry()\n",
    "    \n",
    "#     start_date = start_date_input.value.strip()\n",
    "#     end_date = end_date_input.value.strip()\n",
    "#     cloudPercentage= float(cloud_input.value.strip())\n",
    "#     cloudPercentage = max(0, min(cloudPercentage, 100)) \n",
    "\n",
    "#     mndwiThreshold= float(mndwi_input.value.strip())\n",
    "#     mndwiThreshold = max(0, min(mndwiThreshold, 1)) \n",
    "\n",
    "#     ndviThreshold= float(ndvi_input.value.strip())\n",
    "#     ndviThreshold = max(0, min(ndviThreshold, 1))\n",
    "\n",
    "#     smallThreshold= float(small_input.value.strip())\n",
    "\n",
    "#     bufferThreshold = float(buffer_input.value.strip())\n",
    "#     bufferThreshold = max(0, min(bufferThreshold, 1))\n",
    "   \n",
    "#     customImgs = ee.ImageCollection(\"COPERNICUS/S2_HARMONIZED\") \\\n",
    "#         .filterBounds(latest_aoi) \\\n",
    "#         .filterDate(start_date, end_date) \\\n",
    "#         .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', cloudPercentage)) \n",
    "        \n",
    "#     csPlus = ee.ImageCollection('GOOGLE/CLOUD_SCORE_PLUS/V1/S2_HARMONIZED')\n",
    "#     csPlusBands = csPlus.first().bandNames()\n",
    "#     linkedCol = customImgs.linkCollection(csPlus, csPlusBands)\n",
    "#     filteredMasked = linkedCol.map(maskByCS)\n",
    "#     medianCustomImage=filteredMasked.median().clip(latest_aoi)\n",
    "\n",
    "#     mndwi = medianCustomImage.normalizedDifference([\"B3\" , \"B11\"]).rename(\"MNDWI\")\n",
    "    \n",
    "#     water_nonwater_img = mndwi.gte(mndwiThreshold)\n",
    "\n",
    "#     vectorImage = water_nonwater_img.updateMask(water_nonwater_img.eq(1)).reduceToVectors(**{\n",
    "#         'geometryType':'polygon',\n",
    "#         'reducer':ee.Reducer.countEvery(),\n",
    "#         'scale':10,\n",
    "#         'bestEffort':True,\n",
    "#         'maxPixels':1e8\n",
    "#     })\n",
    "    \n",
    "#     vectorImage = vectorImage.map(lambda feature: feature.set('area', feature.geometry().area(maxError=1000)))\n",
    "\n",
    "#     filtered_vectors = vectorImage.filter(ee.Filter.gte('area', smallThreshold))\n",
    "\n",
    "#     vis_params = {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 3000}\n",
    "#     Map.addLayer(medianCustomImage, vis_params, \"Sentinel-2 RGB\")\n",
    "\n",
    "#     #Map.addLayer(filtered_vectors, {'color': 'red'}, 'Filtered Vectors')\n",
    "\n",
    "#     largeWaterRaster = filtered_vectors.reduceToImage(**{\n",
    "#           'properties': ['area'],\n",
    "#           'reducer': ee.Reducer.first()\n",
    "#       }).gt(0).unmask(0)\n",
    "    \n",
    "#     largeWaterRaster = largeWaterRaster.reproject(\n",
    "#         crs=water_nonwater_img.projection(),  \n",
    "#         scale=10  \n",
    "#     )\n",
    "#     # Map.addLayer(water_nonwater_img.updateMask(water_nonwater_img), {'palette': ['cyan'], 'min': 0, 'max': 1},'Water/Non-Water Classification', False)\n",
    "#     # Map.addLayer(largeWaterRaster.updateMask(largeWaterRaster), {'palette': ['grey','blue']}, 'Large Water Body Raster', True)\n",
    "#     Map.centerObject(latest_aoi, 10)\n",
    "\n",
    "#     landcover_mask = get_landcover_mask()\n",
    "#     if landcover_mask:\n",
    "#         landcoverVisualization = landcover_mask.selfMask()\n",
    "#         landcover_highlight = landcover_mask.gt(0)\n",
    "#         # Map.addLayer(landcoverVisualization.clip(latest_aoi), {\"palette\": [\"red\"]}, \"Selected Landcover Mask\",False)\n",
    "\n",
    "#     landcover_highlight = landcover_highlight.unmask(1)\n",
    "#     minimumSlopeThreshold = 2\n",
    "#     maximumSlopeThreshold = 20\n",
    "#     clip_ned = ned_10m.clip(latest_aoi)\n",
    "#     slope= ee.Terrain.slope(clip_ned)\n",
    "#     landcover_mask = landcover_highlight.gt(0) \n",
    "#     slope_min_mask = slope.lte(minimumSlopeThreshold)  \n",
    "#     slope_max_mask = slope.gte(maximumSlopeThreshold)     \n",
    "#     # Map.addLayer(slope_max_mask.selfMask().clip(latest_aoi), {\"palette\": [\"blue\"]}, \"Slope Max Mask\",False)\n",
    "#     # Map.addLayer(slope_min_mask.selfMask().clip(latest_aoi), {\"palette\": [\"green\"]}, \"Slope Minimum Mask\",False)\n",
    "\n",
    "#     ndvi_index=medianCustomImage.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "#     ndviNonNdvi = ndvi_index.gte(ndviThreshold)\n",
    "#     ndvi1=ndviNonNdvi.eq(1)\n",
    "\n",
    "#     # Map.addLayer(ndvi1.selfMask().clip(latest_aoi), {\"palette\": [\"Pink\"]}, \"NDVI Mask\",False)\n",
    "\n",
    "#     CombinedMask = landcover_mask.Or(slope_min_mask).Or(slope_max_mask).Or(ndvi1)\n",
    "\n",
    "#     bufferSize = bufferThreshold\n",
    "#     bufferMask = CombinedMask.unmask(CombinedMask).focal_max(bufferSize, 'cross','pixels')\n",
    "\n",
    "#     kernel = ee.Kernel.cross(radius=1)\n",
    "#     eroded = largeWaterRaster.focal_min(kernel=kernel, iterations=1)\n",
    "\n",
    "#     edge = largeWaterRaster.subtract(eroded).selfMask()\n",
    "\n",
    "#     CleanEdge = edge.updateMask(bufferMask.eq(0))\n",
    "      \n",
    "\n",
    "#     Map.addLayer(edge, {\"palette\": [\"red\"]}, \"All Edge Pixels\",True)\n",
    "#     Map.addLayer(CleanEdge, {\"palette\": [\"Lime\"]}, \"Clean Edge Pixels\",True)\n",
    "\n",
    "#     geemap.ee_export_image_to_drive(\n",
    "#         CleanEdge, description=\"CleanEdgeGEEMAP\", folder=\"GEEMAPEXPORT\", region=latest_aoi, scale=5\n",
    "#     )\n",
    "  \n",
    "#     # Map.addLayer(CombinedMask.selfMask(), {\"palette\": [\"yellow\"]}, \"Combined\", False)\n",
    "#     # Map.addLayer(bufferMask.selfMask(), {\"palette\": [\"Pink\"]}, \"Buffered\", False)\n",
    "\n",
    "\n",
    "# checkboxes = {name: widgets.Checkbox(value=(True if name in['Open Water','Developed Open Space','Developed Low Intensity','Developed Medium Intensity','Developed High Intensity','Deciduous Forest','Evergreen Forest','Mixed Forest','Cultivated Crops','Woody Wetlands','Emergent Herbaceous Wetlands' ]  else False), description=name) for name in landcover_classes.keys()}\n",
    "\n",
    "\n",
    "# start_date_label = widgets.Label(\"Start Date:\")\n",
    "# start_date_input = widgets.Text(placeholder=\"YYYY-MM-DD\",value='2019-05-26', layout=widgets.Layout(width=\"120px\"))\n",
    "\n",
    "# end_date_label = widgets.Label(\"End Date:\")\n",
    "# end_date_input = widgets.Text(placeholder=\"YYYY-MM-DD\",value='2019-05-28',layout=widgets.Layout(width=\"120px\"))\n",
    "\n",
    "# cloud_label = widgets.Label(\"Image Cloud %:\")\n",
    "# cloud_input = widgets.Text(value=str(100),layout=widgets.Layout(width=\"50px\"))\n",
    "\n",
    "# mndwi_label = widgets.Label(\"MNDWI Threshold\")\n",
    "# mndwi_input = widgets.Text(value=str(.09),layout=widgets.Layout(width=\"50px\"))\n",
    "\n",
    "# ndvi_label = widgets.Label(\"NDVI Threshold\")\n",
    "# ndvi_input = widgets.Text(value=str(.30),layout=widgets.Layout(width=\"50px\"))\n",
    "\n",
    "# buffer_label = widgets.Label(\"Buffer to Combined Mask\")\n",
    "# buffer_input = widgets.Text(value=str(2),layout=widgets.Layout(width=\"50px\"))\n",
    "\n",
    "# small_label = widgets.Label(\"Small Water Body Area Mask\")\n",
    "# small_input = widgets.Text(value=str(30000),layout=widgets.Layout(width=\"100px\"))\n",
    "\n",
    "\n",
    "# update_button = widgets.Button(description=\"Update Map\", button_style=\"primary\")\n",
    "# update_button.on_click(update_map)\n",
    "\n",
    "\n",
    "# clear_button = widgets.Button(description=\"Clear Map\", button_style=\"danger\")\n",
    "\n",
    "\n",
    "# clear_button.on_click(clear_map)\n",
    "\n",
    "\n",
    "# input_row = widgets.HBox([start_date_label, start_date_input, end_date_label, end_date_input, cloud_label, cloud_input])\n",
    "# input_row2 = widgets.HBox([mndwi_label, mndwi_input, ndvi_label, ndvi_input, buffer_label, buffer_input,small_label, small_input])\n",
    "# checkbox_row = widgets.HBox(list(checkboxes.values()))\n",
    "# button_row = widgets.HBox([update_button, clear_button])\n",
    "\n",
    "# ui_box = widgets.VBox([input_row, input_row2, checkbox_row, button_row])\n",
    "\n",
    "# display(ui_box)\n",
    "\n",
    "# Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84523d1e-e4d3-4044-8b97-dc709bbd8d6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Reproject clean flood edge tif file into the same projection and alignment with FLDPLN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b97979-e6e5-4592-8def-a2eee3b712d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snap_raster(src_path, ref_path, dst_path):\n",
    "    '''\n",
    "    This function reprojects and alligns the clean flood edge tiff file into the same projection with FLDPLN library and save the new projected\n",
    "    file.\n",
    "    \n",
    "    args:\n",
    "        src_path (str): Directory of clean flood edge tif file \n",
    "        ref_path (str): Directory of FLDPLN DEM bil file\n",
    "        dst_path (str): Directory of new reprojected clean flood edge file should be \n",
    "    '''\n",
    "    with rasterio.open(ref_path) as ref_src:\n",
    "        ref_transform = ref_src.transform\n",
    "        ref_crs = ref_src.crs\n",
    "        ref_width = ref_src.width\n",
    "        ref_height = ref_src.height\n",
    "        print(ref_transform)\n",
    "        print(ref_height)\n",
    "        print(ref_width)\n",
    "\n",
    "    with rasterio.open(src_path) as src:\n",
    "        profile = src.profile\n",
    "        profile.update({\n",
    "            'crs': ref_crs,\n",
    "            'transform': ref_transform,\n",
    "            'width': ref_width,\n",
    "            'height': ref_height\n",
    "        })\n",
    "\n",
    "        with rasterio.open(dst_path, 'w', **profile) as dst:\n",
    "            for i in range(1, src.count + 1):\n",
    "                reproject(\n",
    "                    source=rasterio.band(src, i),\n",
    "                    destination=rasterio.band(dst, i),\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=ref_transform,\n",
    "                    dst_crs=ref_crs,\n",
    "                    resampling=Resampling.nearest\n",
    "                )\n",
    "    print('New clean flood edge raster saved at: %s'%(dst_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd5dc2-f432-40b7-805f-7e08f2544a59",
   "metadata": {},
   "source": [
    "### Note: If the reprojected clean flood edge raster has not been made, run this cell - src_path is the unaligned clean edge pixels, dst_path is the path to save the new aligned and snapped clean edge pixels, and ref_path is the path to the DEM for that specific library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b54d54-4edc-4708-92ff-2ea5484bde49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_path = str(work_dir / r'C:/.../.tif')\n",
    "# dst_path = str(work_dir / r'C:/.../.tif')    \n",
    "# ref_path = str(work_dir / r'C:/.../dem.bil')\n",
    "# snap_raster(src_path, ref_path, dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a505b-9141-451d-98f5-c09209fde07b",
   "metadata": {},
   "source": [
    "### Else, run the cell below to establish alligned, reprojected clean flood edge raster path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a88b5-f1c8-410a-b657-a923dc71732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_path = str(work_dir / r'C:/...tif')    \n",
    "print(dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3618c9-9c84-4fc4-a3bb-1f0b666135f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Build DataFrame of clean flood edge pixels and their associated index in FLDPLN library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7922d-e8b1-47d2-9a89-65455c5a531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coordinates(raster_path, value):\n",
    "    '''\n",
    "    This function transforms clean flood edge raster into dataframe of X/Y coordinates.\n",
    "    \n",
    "    args:\n",
    "        raster_path (str): Directory of clean flood edge raster (should be reprojected and aligned)\n",
    "        value (int/float): Value that represents clean flood edge (usually 1)\n",
    "\n",
    "    return:\n",
    "        pd.DataFrame\n",
    "    '''\n",
    "    \n",
    "    with rasterio.open(raster_path) as src:\n",
    "        data = src.read(1)\n",
    "        transform = src.transform\n",
    "\n",
    "    rows, cols = np.where(data == value)\n",
    "    xs, ys = rasterio.transform.xy(transform, rows, cols)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'X_coord': xs,\n",
    "        'Y_coord': ys\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def find_tile_id_and_indices(x, y, tile_index_df, cell_size):\n",
    "    '''\n",
    "    This function searchs the tile ID and row/col value of a given FPP X/Y coordinate based on the information \n",
    "    of Max/Min X/Y of each tile.  \n",
    "    \n",
    "    args:\n",
    "        x (int/float): X coordinate (Lon)\n",
    "        y (int/float): Y coordinate (Lat)\n",
    "        tile_index_df (pd.DataFrame): tile information dataframe\n",
    "        cell_size (int/float): spatial resolution in meter\n",
    "\n",
    "    return:\n",
    "        int, int, int: tile ID, FPP col index, FPP row index\n",
    "    '''\n",
    "    for idx, row in tile_index_df.iterrows():\n",
    "        if row['TileMinX'] < x < row['TileMaxX'] and row['TileMinY'] < y < row['TileMaxY']:\n",
    "            fpp_col = int((x - (row['FppMinX'] + hcs)) / cell_size)\n",
    "            fpp_row = int(((row['FppMaxY'] - hcs) - y) / cell_size)\n",
    "            return row['TileId'], fpp_col, fpp_row\n",
    "    return None, None, None\n",
    "\n",
    "def extract_elevation_from_dem(filledDem_path, coordinates_df):\n",
    "    '''\n",
    "    Extract elevation values from a DEM for a given set of coordinates.\n",
    "\n",
    "    args:\n",
    "        dem_path (str): Path to the DEM file.\n",
    "        coordinates_df (pd.DataFrame): DataFrame containing X/Y coordinates (columns: 'X_coord', 'Y_coord').\n",
    "\n",
    "    return:\n",
    "        Input DataFrame with an additional 'FPP_Elevation' column.\n",
    "    '''\n",
    "    with rasterio.open(filledDem_path) as dem:\n",
    "        coords = list(zip(coordinates_df['X_coord'], coordinates_df['Y_coord']))\n",
    "\n",
    "        elevation_values = [\n",
    "            val[0] if val else np.nan \n",
    "            for val in dem.sample(coords)\n",
    "        ]\n",
    "\n",
    "        coordinates_df['FPP_Elevation'] = elevation_values\n",
    "\n",
    "    return coordinates_df\n",
    "\n",
    "value_to_extract = 1\n",
    "RS_edges = extract_coordinates(dst_path, value_to_extract)\n",
    "\n",
    "print(\"Library Folder:\", libFolder)\n",
    "tile_index_path = str(pathlib.Path(libFolder) / allLibNames[0] / 'FLDPLN_tiled_tile_index.csv')\n",
    "print(\"Tile Index Path:\", tile_index_path)\n",
    "tile_index_df = pd.read_csv(tile_index_path)\n",
    "\n",
    "hcs = cell_size / 2\n",
    "tile_index_df.head()\n",
    "\n",
    "RS_edges[['TileId', 'FppCol', 'FppRow']] = RS_edges.apply(\n",
    "    lambda row: pd.Series(find_tile_id_and_indices(row['X_coord'], row['Y_coord'], tile_index_df,cell_size)), axis=1)\n",
    "\n",
    "RS_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b534b3c-f82a-44e2-962c-c720156d8818",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Read tile .snz file and query DTFs relationship of clean flood edge FPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c265c6c0-6a06-4bcc-b146-3117b407ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fspids_and_dtfs_for_coordinate(fpp_col, fpp_row, tile_id, tile_data):\n",
    "    '''\n",
    "    This function saves all FspIDs, DTFs , and the FilledDepth values that correspond to the FPP   \n",
    "    \n",
    "    args:\n",
    "        fpp_col (int): Column index of the FPP in the tile\n",
    "        fpp_row (int): Row index of the FPP in the tile\n",
    "        tile_id (int): The id of the tile\n",
    "        tile_data (dict): Dictionary that contain all tile data from the .snz files \n",
    "\n",
    "    return:\n",
    "        list, list, list: FspID list, Dtf list, FilledDepth list\n",
    "    '''\n",
    "    \n",
    "    tile_df = tile_data.get(tile_id)\n",
    "    if tile_df is not None:\n",
    "        results = tile_df[(tile_df['FppCol'] == fpp_col) & (tile_df['FppRow'] == fpp_row)]\n",
    "        if not results.empty:\n",
    "            return results['FspId'].tolist(), results['Dtf'].tolist(), results['FilledDepth'].tolist()\n",
    "    return [], [], []\n",
    "\n",
    "dummy = RS_edges['TileId'].dropna().unique()\n",
    "unique_tile_ids = dummy.astype(int)\n",
    "print('Tiles to load:'+str(unique_tile_ids))\n",
    "\n",
    "tile_data = {}\n",
    "for tile_id in unique_tile_ids:\n",
    "    snz_path = str(pathlib.Path(libFolder) / allLibNames[0] / str('FLDPLN_tiled_%s.snz'%(tile_id)))\n",
    "    tile_data[tile_id] = pd.read_parquet(snz_path)\n",
    "\n",
    "RS_edges[['FspIds', 'Dtfs', 'FilledDepth']] = RS_edges.apply(\n",
    "    lambda row: pd.Series(get_fspids_and_dtfs_for_coordinate(row['FppCol'], row['FppRow'], row['TileId'], tile_data)), axis=1)\n",
    "\n",
    "RS_edges.head()\n",
    "\n",
    "RS_edges_filtered = RS_edges.copy()\n",
    "RS_edges_filtered['NumsFSP'] = RS_edges['FspIds'].map(len)\n",
    "RS_edges_filtered = RS_edges_filtered.drop(RS_edges_filtered[RS_edges_filtered['NumsFSP'] == 0].index).reset_index(drop = True)\n",
    "RS_edges_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b5738-eded-4de8-9e01-c6ee7e4621b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Create GeoDataframes of the clean flood edge dataframe and of the FSPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b356cd77-0d88-4ae5-9014-bb3b52de4414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdfRS_edges_filtered = gpd.GeoDataFrame(RS_edges_filtered, geometry=gpd.points_from_xy(RS_edges_filtered['X_coord'], RS_edges_filtered['Y_coord']))\n",
    "gdfRS_edges = gpd.GeoDataFrame(RS_edges, geometry=gpd.points_from_xy(RS_edges['X_coord'], RS_edges['Y_coord']))\n",
    "\n",
    "gdfRS_edges_filtered = gdfRS_edges_filtered.set_crs(epsg=26914)\n",
    "\n",
    "max_depth = gdfRS_edges_filtered.Dtfs.apply(lambda x:np.max(x))\n",
    "gdfRS_edges_filtered['max_depth'] = max_depth\n",
    "print(gdfRS_edges_filtered['max_depth'].max())\n",
    "gdfRS_edges_filtered\n",
    "gdfRS_edges_filtered = gdfRS_edges_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f41c86-9c8a-4c73-ba03-bd8e082535a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsp_path = str(pathlib.Path(libFolder) / allLibNames[0] / 'fsp_info.csv')\n",
    "print(libFolder)\n",
    "fsp_info = pd.read_csv(fsp_path)\n",
    "\n",
    "print(fsp_path)\n",
    "fsp_gdf = gpd.GeoDataFrame(fsp_info, geometry=gpd.points_from_xy(fsp_info['FspX'], fsp_info['FspY']))\n",
    "\n",
    "fsp_gdf = fsp_gdf.set_crs(epsg=26914, inplace=True)\n",
    "fspGDFPlot=fsp_gdf\n",
    "fspGDFPlot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c5b61c-b6e9-4bfc-b090-c216cf6b57ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Plot Geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f2e1f2-ba08-4a10-af10-ce90e4a92586",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalize(vmin=gdfRS_edges_filtered['max_depth'].min(), vmax=gdfRS_edges_filtered['max_depth'].max())\n",
    "cmap = plt.cm.viridis\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 6))\n",
    "\n",
    "gdfRS_edges_filtered.plot(column='max_depth', cmap=cmap, markersize=100, ax=ax, legend=False, norm=norm)\n",
    "\n",
    "for x in fsp_gdf['StrOrd'].unique():  \n",
    "    subset = fsp_gdf[fsp_gdf['StrOrd'] == x] \n",
    "    subset.plot(ax=ax, color='green', markersize=2, label=f'Fsp (StrOrd {x})')\n",
    "    \n",
    "for i in unique_tile_ids:\n",
    "    row = tile_index_df.loc[tile_index_df['TileId'] == i, ['TileMinX', 'TileMaxX', 'TileMinY', 'TileMaxY']]\n",
    "    if not row.empty:\n",
    "        minx,maxx,miny,maxy = row.iloc[0]\n",
    "\n",
    "    rectangle = Polygon([(minx, miny), (maxx, miny), (maxx, maxy), (minx, maxy), (minx, miny)])\n",
    "\n",
    "    rec = gpd.GeoDataFrame([1], geometry=[rectangle], crs=\"EPSG:26914\")\n",
    "    rec.plot(ax=ax, edgecolor='black', markersize=1, facecolor='none')\n",
    "\n",
    "sm = ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Max DTF')\n",
    "\n",
    "ax.set_title('Fpps with Max DTF')\n",
    "ax.set_xlabel('X Coordinate (EPSG 26914)')\n",
    "ax.set_ylabel('Y Coordinate (EPSG 26914)')\n",
    "\n",
    "plt.show()\n",
    "gdfRS_edges_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81442b-c727-4788-bedf-31c5bab18413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_user_point_and_fsps(user_point_index,gdf,fsp_gdf):\n",
    "    user_point = gdf.iloc[user_point_index]\n",
    "    related_fsp_ids = user_point['FspIds']\n",
    "    related_fsps = fsp_gdf[fsp_gdf['FspId'].isin(related_fsp_ids)]\n",
    "    \n",
    "    stream_order_colors = {\n",
    "        1: 'darkgreen',\n",
    "        2: 'blue',\n",
    "        3: 'orange',\n",
    "        4: 'purple',\n",
    "        5: 'darkred',\n",
    "        6: 'pink',\n",
    "        7: 'gray',\n",
    "        8: 'black',\n",
    "        9:'royalblue',\n",
    "        10:'brown',\n",
    "        11:'yellow',\n",
    "        12:'darkviolet',\n",
    "        13:'coral',\n",
    "        14:'aqua',\n",
    "        15:'dodgerblue',\n",
    "        16:'grey',\n",
    "        17:'salmon',\n",
    "        18:'olive',\n",
    "        19:'turquoise',\n",
    "        20:'gold',\n",
    "    }\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "    for str_ord, color in stream_order_colors.items():\n",
    "        fsp_subset = fsp_gdf[fsp_gdf['StrOrd'] == str_ord]\n",
    "        fsp_subset.plot(ax=ax, color=color, markersize=2, label=f'Stream Order {str_ord}(Color: {color})')\n",
    "    gdf.plot(ax=ax, color='cyan', markersize=5, label='RS Fpp(Color: Cyan)')\n",
    "    \n",
    "    user_point_gdf = gpd.GeoDataFrame([user_point], geometry=[user_point.geometry])\n",
    "    user_point_gdf.plot(ax=ax, color='red', markersize=50, label=f'User Provided Point (Color: red)')\n",
    "    \n",
    "    related_fsps.plot(ax=ax, color='lawngreen', markersize=25, label='Related Fsp (Color: Lime Green)')\n",
    "    \n",
    "    ax.set_title('Edge Pixels and Related Fsp Points')\n",
    "    ax.set_xlabel('X Coordinate (EPSG 26914)')\n",
    "    ax.set_ylabel('Y Coordinate (EPSG 26914)')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "print(len(gdfRS_edges_filtered))\n",
    "plot_user_point_and_fsps(0,gdfRS_edges_filtered,fspGDFPlot) #Change the number of the pixel here to view which FSPs affect which edge pixels, default is 1\n",
    "fspGDFPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098269e7-ae1c-4e68-91ce-11a925083314",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Assign DOF to FSP from RS clean flood edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332bb785-d7e7-46d2-ae1d-103ac7e44a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfRS_edges_match_full = gdfRS_edges_filtered.copy()\n",
    "n = gdfRS_edges_match_full.NumsFSP.max() \n",
    "\n",
    "gdfRS_edges_match = gdfRS_edges_match_full[gdfRS_edges_match_full.NumsFSP <= n].reset_index(drop = True) \n",
    "gdfRS_edges_match.Dtfs = gdfRS_edges_match.Dtfs.apply(lambda x:np.round(x,1))\n",
    "gdfRS_edges_match.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsp_coord_dict = fsp_gdf[['FspId', 'FspX', 'FspY', 'FilledElev']].set_index('FspId').to_dict('index')\n",
    "def get_fsp_coordinates(fsp_ids):\n",
    "    fsp_coords = []\n",
    "    for fsp_id in fsp_ids:\n",
    "        if fsp_id in fsp_coord_dict:\n",
    "            coords = fsp_coord_dict[fsp_id]\n",
    "            fsp_coords.append((coords['FspX'], coords['FspY']))\n",
    "    return fsp_coords\n",
    "def get_fsp_filledElevations(fsp_ids):\n",
    "    fsp_elevations = []\n",
    "    for fsp_id in fsp_ids:\n",
    "        if fsp_id in fsp_coord_dict:\n",
    "            elevation = fsp_coord_dict[fsp_id]\n",
    "            fsp_elevations.append((elevation['FilledElev']))\n",
    "    return fsp_elevations\n",
    "    \n",
    "gdfRS_edges_match_full['FSP_Coordinates'] = gdfRS_edges_match_full['FspIds'].apply(get_fsp_coordinates)\n",
    "\n",
    "gdfRS_edges_match_full['FSP_Elevation'] = gdfRS_edges_match_full['FspIds'].apply(get_fsp_filledElevations)\n",
    "\n",
    "gdfRS_edges_match_full['FSP_X_Coords'] = gdfRS_edges_match_full['FSP_Coordinates'].apply(lambda x: [coord[0] for coord in x])\n",
    "gdfRS_edges_match_full['FSP_Y_Coords'] = gdfRS_edges_match_full['FSP_Coordinates'].apply(lambda x: [coord[1] for coord in x])\n",
    "\n",
    "gdfRS_edges_match_full = gdfRS_edges_match_full.drop('FSP_Coordinates', axis=1, errors='ignore')\n",
    "\n",
    "gdfRS_edges_match_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9a4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth(fsp,dtf,dof):\n",
    "    '''\n",
    "    This function calculates the flood depth at a certain FPP given the Flood Source Pixel (FSP) that can flood \n",
    "    it, the Depth To Flood (DTF) relationship, and the Depth of Flood (DOF) or stage of the FSP. The flood depth\n",
    "    is calculated as the maximum of (DOF - DTF) for each FSP. \n",
    "    \n",
    "    If flood depth is less than 0 (no FSP can flood the FPP), return nan\n",
    "    \n",
    "    args:\n",
    "        fsp (np.arary/list): FSP list\n",
    "        dtf (np.arary/list): corresponding DTF of the FSP\n",
    "        dof (np.array/list): corresponding DOF/stage of the FSP\n",
    "        \n",
    "    return:\n",
    "        float: flood depth value\n",
    "        nan: if FPP is not flooded\n",
    "    '''\n",
    "    fsp, dtf, dof = np.array(fsp), np.array(dtf), np.array(dof)\n",
    "    \n",
    "    depth = np.max(dof - dtf)\n",
    "    \n",
    "    if depth >= 0:\n",
    "        return depth\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def filter_close_values(input_dict, threshold):\n",
    "    '''\n",
    "    This function reduces the number of options for each FSPs to reduce computational time. Given a certain threshold, \n",
    "    no two values within each list are closer than the threshold.\n",
    "    \n",
    "    args:\n",
    "        input_dict (dict): Dictionary of key:value being FspID (int): Dof options (list)\n",
    "        threshold (float): Threshold to remove values close to each other\n",
    "    \n",
    "    return:\n",
    "        dict: filtered out FSP DOF \n",
    "    '''\n",
    "    \n",
    "    filtered_dict = {}\n",
    "    for key, values in input_dict.items():\n",
    "        # Sort values in descending order\n",
    "        sorted_values = sorted(values, reverse=True)\n",
    "        filtered_values = []\n",
    "        for value in sorted_values:\n",
    "            if all(abs(value - fv) > threshold for fv in filtered_values):\n",
    "                filtered_values.append(value)\n",
    "        \n",
    "        filtered_dict[key] = filtered_values\n",
    "    \n",
    "    return filtered_dict\n",
    "\n",
    "def num_of_combination(possible_fsp_dtf):\n",
    "    '''\n",
    "    This function calculate the number of possible combinations for FSP assignments \n",
    "    \n",
    "    args:\n",
    "        possible_fsp_dtf (dict): Dictionary of key:value being FspID (int): Dof options (list)\n",
    "        \n",
    "    return:\n",
    "        int: number of combinations\n",
    "    '''\n",
    "    a = 1\n",
    "    for key, value in possible_fsp_dtf.items():\n",
    "        a = a*len(value)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc2400",
   "metadata": {},
   "source": [
    "## Go through all possible combinations\n",
    "### Note: We use the set of clean flood edge with the lowest NumofFSP to assign associated FSP DOFs and then iteratively move on to the set of clean flood edge with higher NumofFSP. FSPs being assigned in each iteration doesn't change after each iteration. \n",
    "\n",
    "### Set the maximum number of possible combinations for each iteration. When the total number of possible combinations exceed this value, the FSP DOF dictionary is filtered with a 0.1 incremental threshold to remove DOFs options that are close to each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64fe4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_combinations = 100000 # The maximum number of combinations to avoid excessive copmutation, if there are a lot of edge pixels and computation times are too long reduce this number\n",
    "best_case = {}\n",
    "for i in range(gdfRS_edges_match_full.NumsFSP.min(),n + 1): \n",
    "    print(\"NumofFSPs <= %s\"%(i))\n",
    "\n",
    "    gdfRS_edges_match = gdfRS_edges_match_full[gdfRS_edges_match_full.NumsFSP <= i].reset_index(drop = True) \n",
    "    gdfRS_edges_match.Dtfs = gdfRS_edges_match.Dtfs.apply(lambda x:np.round(x,1))\n",
    "    \n",
    "    #Generate Possible DTF Values for Each FSP \n",
    "    possible_fsp_dtf = defaultdict(set)\n",
    "    for fspids, dtfs in zip(gdfRS_edges_match['FspIds'], gdfRS_edges_match['Dtfs']): \n",
    "        for fspid, dtf in zip(fspids, dtfs):\n",
    "            possible_fsp_dtf[fspid].add(dtf)\n",
    "    \n",
    "    possible_fsp_dtf = {k: list(v) for k, v in possible_fsp_dtf.items()}\n",
    "    print('Total number of possible Fsp are %s'%(len(possible_fsp_dtf)))\n",
    "\n",
    "    for key,value in best_case.items():\n",
    "        possible_fsp_dtf[key] = [value]\n",
    "\n",
    "    num_choice = num_of_combination(possible_fsp_dtf)\n",
    "    print('There are %s options of FSP DOF '%(num_choice))\n",
    "\n",
    "    #Reduce options if the number of combinations exceeds the threshold \n",
    "    threshold = 0.1\n",
    "    while num_choice > max_combinations:\n",
    "        possible_fsp_dtf = filter_close_values(possible_fsp_dtf, threshold)\n",
    "        num_choice = num_of_combination(possible_fsp_dtf)\n",
    "        print('Applied %s threshold. There are %s options of FSP DOF '%(np.round(threshold, 1), num_choice))\n",
    "        threshold += 0.1\n",
    "\n",
    "    #Generarte all possible combinations of FSP-DOF values\n",
    "    fsp = list(possible_fsp_dtf.keys())\n",
    "    dtf_options = list(possible_fsp_dtf.values())\n",
    "    combinations = [dict(zip(fsp, combo)) for combo in product(*dtf_options)]\n",
    "\n",
    "    total_edge_depth_optimize = [np.inf] \n",
    "    idx_optimize = np.nan \n",
    "    \n",
    "    # Find the best combination that minimize flood depth at edge\n",
    "    for i in range(len(combinations)):\n",
    "        total_edge_depth = []\n",
    "        edge_condition = True\n",
    "        \n",
    "        for j in range( gdfRS_edges_match.shape[0]): \n",
    "            fsp = gdfRS_edges_match.loc[j, 'FspIds']\n",
    "            dtf = gdfRS_edges_match.loc[j, 'Dtfs']  \n",
    "            dof = [combinations[i][key] for key in fsp]\n",
    "\n",
    "            flood_depth = get_depth(fsp,dtf,dof)\n",
    "            if np.isnan(flood_depth):\n",
    "                edge_condition = False\n",
    "                break\n",
    "            else:\n",
    "                total_edge_depth.append(flood_depth)\n",
    "  \n",
    "        if not edge_condition: \n",
    "            continue\n",
    "        else:\n",
    "            # Update optimal combination if current one has lower total flood depth \n",
    "            if sum(total_edge_depth) < sum(total_edge_depth_optimize):\n",
    "                total_edge_depth_optimize = total_edge_depth\n",
    "                idx_optimize = i\n",
    "\n",
    " if not np.isnan(idx_optimize):\n",
    "    best_case = combinations[idx_optimize]\n",
    "    print('%s FSP optimized' %(len(best_case)))\n",
    "    print('Edge pixels that dont get flooded: %s'%(np.count_nonzero(np.isnan(total_edge_depth_optimize))))\n",
    "    print('Minimum flood depth at edge pixels: %s'%(np.nanmin(total_edge_depth_optimize).round(2)))\n",
    "    print('Mean flood depth at edge pixels: %s'%(np.nanmean(total_edge_depth_optimize).round(2)))\n",
    "    print('Max flood depth at edge pixels: %s'%(np.nanmax(total_edge_depth_optimize).round(2)))\n",
    "    print('-------------------------------------------------------------------------------------------')"
    " else:\n",
    "         print("No valid combination found for i =", i)\n",
    "         continue\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e242b-9a97-4fb2-9786-b7b925522bee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SNAP to FSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6648e-87cc-458e-8f6f-9d21a43f1592",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = fsp_gdf.set_index('FspId').loc[best_case.keys()]\n",
    "sol_Use = sol\n",
    "sol_Use['Dof'] = sol_Use.index.map(best_case)\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  \n",
    "    display(sol_Use)\n",
    "\n",
    "print(sol_Use['StrOrd'].value_counts())\n",
    "\n",
    "highest_value = sol_Use['StrOrd'].value_counts().index[0]\n",
    "print('Highest value =',highest_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2dc1a8-8c27-4b56-83bc-0e3405d8fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_Use = sol_Use[(sol_Use['StrOrd'] == highest_value)] # replace highest_value with any stream order number to select a specific stream order or comment out this line to use all stream orders. Can change results drastically. Make sure stream order where flooding is occuring from is selected. Or comment out to include all tributaries in flooding\n",
    "sol_Use=sol_Use.reset_index()\n",
    "gaugeFspDf = sol_Use[['FspId','FspX', 'FspY', 'StrOrd', 'DsDist', 'SegId', 'FilledElev','Dof']].reset_index(drop = True)\n",
    "gaugeFspDf.insert(0, 'lib_name', allLibNames[0])\n",
    "gaugeFspDf.to_csv('gaugeFspDf.csv', index=False)\n",
    "display(gaugeFspDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e45fd-3c89-4d26-ae90-2fb01852dda5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Interpolate FSP's DOF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a2c97-d037-46d4-9612-1cf2f9841c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find libs with snapped gauges. They are the actual libs to map\n",
    "libs2Map = gaugeFspDf['lib_name'].drop_duplicates().tolist()\n",
    "print(libs2Map)\n",
    "\n",
    "# prepare the DF for storing interpolated FSP DOF\n",
    "fspDof = pd.DataFrame(columns=['LibName','FspId','Dof'])\n",
    "\n",
    "# prepare DFs for saving interpolated FSPs and their segment IDs\n",
    "fspCols = fspInfoColumnNames + ['Dof']\n",
    "segIdCols = ['SegId','LibName']\n",
    "fsps = pd.DataFrame(columns=fspCols)\n",
    "segIds =pd.DataFrame(columns=segIdCols)\n",
    "\n",
    "# map each library\n",
    "for libName in libs2Map:\n",
    "    # interpolate DOF for the gauges\n",
    "    fspIdDof = InterpolateFspDofFromGauge(libFolder,libName,gaugeFspDf,weightingType='V') # vertical 'V' by default can use horizontal as well 'H'\n",
    "    fspIdDof['LibName'] = libName\n",
    "    fspIdDofUse=fspIdDof.dropna()\n",
    "    display(fspIdDofUse)\n",
    "\n",
    "    # fspDof = fspDof.append(fspIdDof[['LibName','FspId','Dof']], ignore_index=True)\n",
    "    fspDof = pd.concat([fspDof,fspIdDofUse[['LibName','FspId','Dof']]], ignore_index=True)\n",
    "    print(len(fspDof))\n",
    "    \n",
    "    if len(fspDof) < 1000:\n",
    "    # Round to the lowest hundredth plus one\n",
    "        window_size = (len(fspDof) // 100) * 100 + 1\n",
    "    else:\n",
    "        # Round to the lowest thousandth plus one\n",
    "        window_size = (len(fspDof) // 1000) * 1000 + 1\n",
    "\n",
    "    # Ensure the window size is odd\n",
    "    if window_size % 2 == 0:\n",
    "        window_size -= 1\n",
    "\n",
    "    print(window_size)\n",
    "    fspDof['Dof'] = savgol_filter(fspDof['Dof'], window_size, 2)\n",
    "    # Keep interpolated FSP DOF for saving later\n",
    "    fspFile = os.path.join(libFolder, libName, fspInfoFileName)\n",
    "    fspDf = pd.read_csv(fspFile) \n",
    "    fspDf = pd.merge(fspDf,fspDof,how='inner',on=['FspId'])\n",
    "    # fsps = fsps.append(fspDf, ignore_index=True)\n",
    "    fsps = pd.concat([fsps,fspDf], ignore_index=True)\n",
    "    \n",
    "    # Keep FSP segment IDs for saving later\n",
    "    t =  pd.DataFrame(fspDf['SegId'].drop_duplicates().sort_values())\n",
    "    t['LibName'] = libName\n",
    "    # segIds = segIds.append(t, ignore_index=True)\n",
    "    segIds = pd.concat([segIds,t], ignore_index=True)\n",
    "\n",
    "\n",
    "# show interpolated FSPs with Dof\n",
    "print(fspDof)\n",
    "#\n",
    "# save interpolated FSP DOF and their segments for checking. This block of code should be commented out if no-checking needed\n",
    "#\n",
    "# Save DOF and segment IDs to CSV files\n",
    "FspDofFile = os.path.join(outputFolder, 'Interpolated_FSP_DOF.csv')\n",
    "SegIdFile = os.path.join(outputFolder, 'Interpolated_SegIds.csv')\n",
    "\n",
    "fsps.to_csv(FspDofFile, index=False)\n",
    "segIds.to_csv(SegIdFile, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d5e5894-252e-49e3-857b-affc4a7c8a46",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Map Flood Inundation Depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0248ea1-b0b9-48ad-a093-2de1c55fc6f4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The process of generating inundation depth map happens here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64feb7-864d-46cf-98c6-d695e9681848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show mapping info\n",
    "print(f'Tiled FLDPLN library folder: {libFolder}')\n",
    "print(f'Map folder: {outMapFolder}')\n",
    "# Find libs needs mapping\n",
    "libs2Map = fspDof['LibName'].drop_duplicates().tolist()\n",
    "print(f'Libraries to map: {libs2Map}')\n",
    "\n",
    "# check running time\n",
    "startTimeAllLibs = time.time()\n",
    "\n",
    "# create a local cluster to speed up the mapping. Must be run inside \"if __name__ == '__main__'\"!!!\n",
    "if useLocalCluster:\n",
    "    # cluster = LocalCluster(n_workers=4,processes=False)\n",
    "    try:\n",
    "        print('Start a LocalCluster ...')\n",
    "        # NOTE: set worker space (i.e., local_dir) to a folder that the LocalCluster can access. When run the script through a scheduled task, \n",
    "        # the system uses C:\\Windows\\system32 by default, which a typical user doesn't have the access!\n",
    "        # cluster = LocalCluster(n_workers=numOfWorkers,memory_limit='32GB',local_dir=\"D:/projects_new/fldpln/tools\") # for KARS production server (192G RAM & 8 cores)\n",
    "        # cluster = LocalCluster(n_workers=numOfWorkers,processes=False) # for KARS production server (192G RAM & 8 cores)\n",
    "        cluster = LocalCluster(n_workers=numOfWorkers,memory_limit='8GB',local_dir=\"D:\\temp\") # for office desktop (64G RAM & 8 cores)\n",
    "        # print('Watch workers at: ',cluster.dashboard_link)\n",
    "        print(f'Number of workers: {numOfWorkers}')\n",
    "        client = Client(cluster)\n",
    "        # print scheduler info\n",
    "        # print(client.scheduler_info())\n",
    "    except:\n",
    "        print('Cannot create a LocalCLuster!')\n",
    "        useLocalCluster = False\n",
    "\n",
    "# dict to store lib processing time\n",
    "libTime={}\n",
    "\n",
    "# map each library\n",
    "for libName in libs2Map:\n",
    "    # check running time\n",
    "    startTime = time.time()\n",
    "    \n",
    "    print(fspDof)\n",
    "    # select the FSPs within the lib\n",
    "    fspIdDof = fspDof[fspDof['LibName']==libName][['FspId','Dof']]\n",
    "    print(fspIdDof)\n",
    "\n",
    "    # mapping flood depth\n",
    "    if useLocalCluster:\n",
    "        print(f'Map [{libName}] using LocalCLuster ...')\n",
    "        # generate a DAG\n",
    "        dag,dagRoot=MapFloodDepthWithTilesAsDag(libFolder,libName,'snappy',outMapFolder,fspIdDof,aoiExtent=None)\n",
    "        if dag is None:\n",
    "            tileTifs = None\n",
    "        else:\n",
    "            # visualize DAG\n",
    "            # visualize(dag)\n",
    "            # Compute DAG\n",
    "            tileTifs = client.get(dag, dagRoot)\n",
    "            if not tileTifs: # list is empty\n",
    "                tileTifs =  None\n",
    "    else:\n",
    "        print(f'Map {libName} ...')\n",
    "        tileTifs = MapFloodDepthWithTiles(libFolder,libName,'snappy',outMapFolder,fspIdDof,aoiExtent=None)\n",
    "    print(f'Actual mapped tiles: {tileTifs}')\n",
    "\n",
    "    # Mosaic all the tiles from a library into one tif file\n",
    "    if mosaicTiles and not(tileTifs is None):\n",
    "        print('Mosaic tile maps ...')\n",
    "        mosaicTifName = libName+'_'+outMapFolderName+'.tif'\n",
    "        # Simplest implementation, may crash with very large raster\n",
    "        MosaicGtifs(outMapFolder,tileTifs,mosaicTifName,keepTifs=False)\n",
    "    \n",
    "    # check time\n",
    "    endTime = time.time()\n",
    "    usedTime = round((endTime-startTime)/60,3)\n",
    "    libTime[libName] = usedTime\n",
    "    # print(f'{libName} processing time (minutes):', usedTime)\n",
    "\n",
    "# Show processing time\n",
    "# Individual lib processing time\n",
    "print('Individual library mapping time:', libTime)\n",
    "# total time\n",
    "endTimeAllLibs = time.time()\n",
    "print('Total processing time (minutes):', round((endTimeAllLibs-startTimeAllLibs)/60,3))\n",
    "\n",
    "#\n",
    "# Shutdown local clusters\n",
    "#\n",
    "if useLocalCluster:\n",
    "    print('Shutdown LocalCluster ...')\n",
    "    cluster.close()\n",
    "    client.shutdown()\n",
    "    client.close()\n",
    "    useLocalCluster = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
